# INTRODUCTION
> Unlike classification and regression, clustering is an unsupervised learning task. Clustering is to partition the data set into different clusters by some measure, and ensure that the similarity in the same cluster is as large as possible, and the similarity in different clusters is as small as possible.
Clustering methods can be divided into partition clustering methods, density-based clustering methods and hierarchical clustering methods. This paper will summarize the classical algorithms for each clustering method.

| Partition | Density | Hierarchical | Other |
|--|--|--|--|
| k-means | DBSCAN | Agglomerative | Kernel clustering |
| k-means++ | OPTICS | Divisive | Gaussian mixture model clustering |
| bi-kmeans |
# Partitioning clustering
## 1. k-means
### 1.1. CONCEPT
K-means clustering first randomly selects k points as the initial cluster centers, the points closest to these centers are divided into the same cluster, and this step is repeated until the cluster centers are no longer updated or the maximum number of iterations has been reached. The distance metric used in K-means is Euclidean distance, and the k value can be selected using the elbow method or Gap Statistic.
### 1.2. ADVANTAGE
1. Easy to understand and implement;
2. For large data sets, its computational complexity is not high, close to linear;
3. Strong interpretability;
4. Only one hyperparameter k needs to be tuned.
### 1.3. DISADVANTAGE
1. The initial value of K needs to be determined in advance. If not chosen properly, it will lead to wrong clustering;
2. It can only converge to a local optimum, and its clustering effect is affected by the initial value;
3. Sensitive to noise;
4. The points can only be classified into a single class.
### 1.4. APPLYING
k-means is suitable for processing large data sets with spherical clusters and no outliers. It is not suitable for non-spherical clusters, clusters of different sizes and different densities, that is, data with different data distributions.
```python
import numpy as np 
from sklearn.cluster import KMeans 
from matplotlib import pyplot

x = np.array([ [1,2],[1.5,1.8],[5,8],[8,8],[1,0.6],[9,11] ])
clf = KMeans(n_clusters=2) 
clf.fit(x)
centers = clf.cluster_centers_ # center of clustering
labels = clf.labels_ # The cluster to which each data point belongs
# print(centers) 
# print(labels)
# plot centers and data points
for i in range(len(labels)):
	pyplot.scatter(x[i][0], x[i][1], c=('r' if labels[i] == 0 else 'b'))
pyplot.scatter(centers[:,0],centers[:,1],marker='*', s=100)
# predict
predict = [[2,1], [6,9]]
label = clf.predict(predict)
# plot predictions and their clusters
for i in range(len(label)): 
	pyplot.scatter(predict[i][0], predict[i][1], c=('r' if label[i] == 0 else 'b'), marker='x')
pyplot.show()
```
## 2. k-means++
### 2.1. CONCEPT
The main difference between k-means++ and k-means is that K-means ++ improves the way K-means selects cluster centers, and the sample points that are further away from other cluster centers are more likely to be selected as the next cluster center. For each sample point, the probability of being selected as the next cluster center, P(x), is calculated as follows, where D(x) is the shortest distance between each sample point and the current cluster center.
![输入图片说明](/imgs/2022-12-15/djlhFGyFLHUM5tcM.png)
# Density-based clustering
## 1. DBSCAN
### 1.1. CONCEPT
If there are at least M objects in the neighborhood of p, create a new cluster C, traverse the neighborhood N of object p, if the point p' in the neighborhood is unlabeled, mark it first, if it does not belong to any other cluster, add p' to C, for each p' also add the point in its neighborhood to N. The above process is repeated until all points are marked.
### 1.2. ADVANTAGE
1. Good noise immunity;
2. Can handle clusters of any shape and size;
3. You don't need to specify the number of clusters beforehand;
4. Less susceptible to outliers.
### 1.3. DISADVANTAGE
1. When the density of clusters does not change much or the density distribution is uneven, the clustering effect is not good;
2. For high-dimensional data, it is difficult to define the density;
3. The algorithm is complex, and the distance threshold and neighborhood sample threshold need to be adjusted.
### 1.4. APPLYING
DBSCAN algorithm is suitable for processing medium and low dimensional data sets with different density clusters.
```python
from sklearn.cluster import DBSCAN
import numpy as np
X = np.array([[1, 2], [2, 2], [2, 3], [8, 7], [8, 8], [25, 80]])
clustering = DBSCAN(eps=3, min_samples=2).fit(X) # eps is the neighbor threshold, and min_samples is the neighbor numbers of center object
print(clustering.labels_)
print(clustering.get_params)
X1 = np.array([[3,3], [3,4], [5,4]])
X1_pred = clustering.fit_predict(X1)
```
# Hierarchical clustering
## 1. Agglomerative
### 1.1. CONCEPT
Initially, each input sample forms a cluster, and each cluster contains only one element. Each time, the two closest clusters are merged to form a new cluster according to certain rules, and the process is repeated until all the objects belong to the same cluster. Hierarchical clustering measures the proximity between clusters in three ways: single chain, full chain and group averaging.

Single link: The shortest distance (maximum similarity) between any two points in two different clusters. Suitable for non-elliptical clusters, but sensitive to noise and outliers.

Full chain: The longest distance (minimum similarity) between any two points in two different clusters. It is insensitive to noise and outliers, but it may break large clusters, and it prefers spherical shapes.

Group average: The average of the proximity of all pairs of points in different clusters.
### 1.2. ADVANTAGE
1. Produces high-quality clusters;
2. Be able to discover class hierarchies;
3. Can handle clusters of any shape or size.
### 1.3. DISADVANTAGE
1. Lacks a global objective function, as each step locally determines which clusters should be merged, tending to make good local decisions;
2. Large amount of computation and storage.
### 1.4. APPLYING
Agglomerative works well for small data sets.
```python
import numpy as np
from sklearn.cluster import AgglomerativeClustering
import random
import matplotlib.pyplot as plt
from scipy.cluster.hierarchy import dendrogram, linkage

point_random = []
for i in range(10):
    rx = random.uniform(0, 10)
    ry = random.uniform(0, 10)
    point_random.append([rx, ry])

group_size = 3
cls = AgglomerativeClustering(n_clusters=group_size,linkage='ward')
cluster_group = cls.fit(np.array(point_random))
cnames = ['black', 'blue', 'red']
for point, gp_id in zip(point_random, cluster_group.labels_):
    plt.scatter(point[0], point[1], s=5, c=cnames[gp_id], alpha=1)
plt.show()
```
