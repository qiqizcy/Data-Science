# INTRODUCTION
> Artificial neural network (ANN), also referred to as neural network, is a subset of machine learning and the core of deep learning algorithm. It is a mathematical model of distributed parallel information processing which imitates the behavior characteristics of animal neural network. This kind of network depends on the complexity of the system, by adjusting the interconnection between a large number of internal nodes, so as to achieve the purpose of processing information.
>This paper will focus on the classical neural network models of feedforward neural network and recurrent neural network, whose structure is shown in the figure below.
>![输入图片说明](/imgs/2022-12-17/A0ZgSCThex5qOS00.png)

# Deep feedforward networks
## 1. MLP
### 1.1. CONCEPT
Multilayer perceptron contains input layer, hidden layer and output layer, and the layers are fully connected to each other. The hidden layer can have one or many layers, the simplest multilayer perceptron has only one hidden layer.

If there are n-ary inputs, the perceptron with single hidden layer needs 2^(n-1) hidden nodes to implement. For perceptron with multiple hidden layers, 3(n-1) hidden nodes are needed.

The multi-layer perceptron uses the back propagation algorithm to update the parameters, and the overall cost function used in the calculation includes the loss function and the L2 regularization term of the parameters. The loss function can be divided into square error loss function and cross entropy loss function according to different task types. The former is more suitable for regression problems, that is, neural networks with continuous output and no Sigmoid or Softmax activation function in the last layer. The latter is more suitable for binary or multiclass classification problems, where the outputs are discrete and the last layer often uses a Sigmoid activation function. Multilayer perceptron contains input layer, hidden layer and output layer, and the layers are fully connected to each other. The hidden layer can have one or many layers, the simplest multilayer perceptron has only one hidden layer.

If there are n-ary inputs, the perceptron with single hidden layer needs 2^(n-1) hidden nodes to implement. For perceptron with multiple hidden layers, 3(n-1) hidden nodes are needed.

The multi-layer perceptron uses the back propagation algorithm to update the parameters, and the overall cost function used in the calculation includes the loss function and the L2 regularization term of the parameters. The loss function can be divided into square error loss function and cross entropy loss function according to different task types. The former is more suitable for regression problems, that is, neural networks with continuous output and no Sigmoid or Softmax activation function in the last layer. The latter is more suitable for binary or multiclass classification problems, where the outputs are discrete and the last layer often uses a Sigmoid activation function.
### 1.2. ADVANTAGE
1. Nonlinear activation functions can be used to learn nonlinear models and produce more powerful learning and fitting capabilities than linear models.
### 1.3. DISADVANTAGE
1. MLP is sensitive to feature scaling;
2. When the number of neurons increases, the fully connected structure of MLP leads to large storage and computation overhead;
3. When the number of hidden layers is too large, the gradient may disappear in hidden layers close to the input layer;
4. With the increase of hidden layers, the optimization function is easy to fall into local optimal solutions;
5. Unable to work with time series data.
### 1.4. APPLYING
Multilayer perceptron is suitable for solving classification or regression problems with unordered input and small amount of data.
```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import classification_report,confusion_matrix

data = pd.read_csv('your file')
X_train, X_test, y_train, y_test = train_test_split(X, y)
scaler = StandardScaler()
scaler.fit(X_train)
X_train = scaler.transform(X_train)
X_test = scaler.transform(X_test)

mlp = MLPClassifier(hidden_layer_sizes=(13, 13, 13), max_iter=500)
mlp.fit(X_train, y_train)
predictions = mlp.predict(X_test)

print(confusion_matrix(y_test, predictions))
print(classification_report(y_test, predictions))
len(mlp.coefs_)
len(mlp.coefs_[0])
len(mlp.intercepts_[0])
```
## 2. CNN
### 2.1. CONCEPT
CNN usually consists of an input layer, a number of convolutional layers, pooling layers, a number of fully connected layers and an output layer. Its characteristic is that the neuron nodes of each layer only respond to the neurons in the local area of the previous layer (each neuron node in the fully connected network responds to all nodes in the previous layer), and CNN also uses the back propagation algorithm to update parameters.

The convolution operation of CNN has sparse interaction and parameter sharing, and parameter sharing makes the convolutional layer have translation equivariant. The pooling operations include mean pooling, Max pooling, pooling of adjacent overlapping regions and spatial pyramid pooling.
### 2.2. ADVANTAGE
1. The parameter sharing feature of convolution operation greatly reduces the number of parameters, reduces the storage and computational overhead of the model, and improves the efficiency and scalability of the model;
2. The pooling operation significantly reduces the number of parameters and is invariant to translation, scaling, and rotation;
3. Convolution and pooling operations can automatically perform feature extraction;
4. Can learn temporal information in fixed-length sequence data.
### 2.3. DISADVANTAGE
1. When the number of network layers is too deep, the parameters of the network layers close to the input layer are updated slowly, and even the gradient disappears;
2. With the deepening of the number of network layers, the optimization function is easy to fall into local optimal solutions;
3. The pooling layer will lose some valuable information and ignore the correlation between local and global.
### 2.4. APPLYING
Since the convolution operation mainly deals with the data of network structure, CNN is often used for the analysis and recognition of images and time series data. CNN can also be used for text classification to obtain semantic information at different levels of abstraction. CNNS are suitable for processing high-dimensional data.
```python
#importing the required libraries
from tensorflow.keras.datasets import mnist
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D
from tensorflow.keras.layers import MaxPool2D
from tensorflow.keras.layers import Flatten
from tensorflow.keras.layers import Dropout
from tensorflow.keras.layers import Dense

#loading data
(X_train,y_train) , (X_test,y_test)=mnist.load_data()
#reshaping data
X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], X_train.shape[2], 1))
X_test = X_test.reshape((X_test.shape[0],X_test.shape[1],X_test.shape[2],1)) 
#checking the shape after reshaping
print(X_train.shape)
print(X_test.shape)
#normalizing the pixel values
X_train=X_train/255
X_test=X_test/255

#defining model
model=Sequential()
#adding convolution layer
model.add(Conv2D(32,(3,3),activation='relu',input_shape=(28,28,1)))
#adding pooling layer
model.add(MaxPool2D(2,2))
#adding fully connected layer
model.add(Flatten())
model.add(Dense(100,activation='relu'))
#adding output layer
model.add(Dense(10,activation='softmax'))
#compiling the model
model.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['accuracy'])
#fitting the model
model.fit(X_train,y_train,epochs=10)

# evaluating the model 
model.evaluate(X_test,y_test)
```
## 3. ResNet
### 3.1. CONCEPT
In order to solve or alleviate the vanishing gradient problem in deep neural network training, ResNet adjusts the network architecture to short the network layers closer to the input to the layers closer to the output. For example, the output function of input x through two neural networks is F(x), then the objective function is H(x)=F(x)+x, then F(x) only needs to fit the residual between the objective function and the input. Since ResNet learns the residuals between outputs and inputs rather than the outputs of each layer, adding more hidden layers will not make the model worse even if it already has a good fit.
### 3.2. ADVANTAGE
1. Greatly improved the number of deep neural networks that can be trained effectively;
2. Reduces the problem of gradient disappearance in network layers close to the input layer;
3. Simplify the learning objective and difficulty of neural network.
### 3.3. DISADVANTAGE
1. Unable to work with time series data.
### 3.4. APPLYING
ResNet is suitable for large capacity models and is suitable for processing more complex data structures such as images, text, etc.
# Recurrent neural networks
## 1. TRNN
### 1.1. CONCEPT
In addition to the output of the previous layer, the hidden layer of the traditional RNN also contains a hidden state, which is obtained by encoding the information of the previous t inputs, and can be calculated by the current input and the hidden state of the previous layer of the neural network. The states in the last layer encode information about the whole sequence. RNNS solve for the model parameters using BPTT (Back Propagation Through Time) algorithm, which is a simple variant of the back propagation algorithm.
### 1.2. ADVANTAGE
1. Works well with long, ordered input sequences of text data.
### 1.3. DISADVANTAGE
1. The BPTT algorithm used by RNN is prone to the problem of gradient disappearance and gradient explosion in deep neural networks, and its learning ability is limited.
### 1.4. APPLYING
Traditional RNN can be used for text classification, in which the hidden layer activation function usually uses Tanh function or ReLU function, and the output layer activation function usually uses Softmax function.
## 2. LSTM
### 2.1. CONCEPT
Compared with traditional RNN, LSTM incorporates input gate, forget gate and output gate. The input gate controls to what extent the new state of the current computation is updated into the memory cell. The forgetting gate controls how much information in the previous memory cell is forgotten. The output gate controls how much the current output depends on the current memory cell. In a trained LSTM model, when there is no important information in the input sequence, the value of the forgetting gate of LSTM is close to 1, and the value of the input gate is close to 0, at this time, the past memory will be saved, and the new information will not be remembered. When there is important information in the input sequence, the value of the input gate is close to 1; When important information appears in the input sequence and this information means that the previous memory is no longer important, the value of the input gate is close to 1 and the value of the forget gate is close to zero, so that the new important information is memorized and the old memory is forgotten, saving storage space. Since the physical definition of the gating unit is generally 0 to 1, the activation functions of the input, forget and output gates are usually Sigmoid functions.
### 2.2. ADVANTAGE
1. LSTM can perform long-term memory of valuable information, thereby reducing the learning difficulty of RNN;
2. LSTM avoids the vanishing gradient problem of traditional RNN.
### 2.3. DISADVANTAGE
1. When the time range of the sequence is long and the network depth is large, the computation of LSTM is large and time-consuming;
2. Although the gradient problem of RNNS is somewhat solved in LSTMS, sequences over 1000 magnitudes still do not handle well.
### 2.4. APPLYING
LSTM is suitable for processing sequential data and are commonly used in speech recognition, language modeling, machine translation, named entity recognition, and image captioning text generation.
## 3. Seq2Seq
### 3.1. CONCEPT
The core idea of Seq2Seq model is to map an input sequence to an output sequence through a deep neural network. This process consists of two steps: encoding input and decoding output. The input sequence is encoded by the encoder and mapped to a set of vectors, and then decoded by the decoder to obtain the output vector. Among them, both encoder and decoder are implemented using RNN, which can be chosen as traditional RNN or LSTM. The core component of a Seq2Seq model is its decoding component, which can be solved either using greedy methods (which often lead to local optima) or using beam search. Other improvements for decoding include using stacked RNNS, adding Dropout mechanisms, establishing residual connections with the encoder, etc.
### 3.2. ADVANTAGE
1. Seq2Seq improves the ability to process long sentences compared to LSTM.
### 3.3. DISADVANTAGE
1. The longer the input sequence, the more likely it is that the previous data in the sequence will be lost;
2. The decoder fails to align the encoder.
### 3.4. APPLYING
Seq2Seq is commonly used in machine translation, speech recognition, automatic dialogue and other tasks.
## 4. Seq2Seq(Attention)
### 4.1. CONCEPT
The traditional Seq2Seq model often loses part of the input information because all the information of the input sequence is compressed into a vector representation during encoding. Therefore, the attention mechanism is considered to be added to the Seq2Seq model. When the attention mechanism is added, each output depends on the previous hidden state and each corresponding hidden state of the input sequence (obtained by calculating the weighted sum of all hidden states of the input sequence). The attention weight parameter α in the weighted sum is not a fixed weight, but is calculated according to the encoder neural network. In addition, in order to prevent the loss of text information, the bidirectional recurrent neural network can be used for encoding.
### 4.2. ADVANTAGE
1. Bidirectional recurrent neural network avoids the loss of contextual information and makes the model output more accurate;
2. After adding the attention mechanism, the complexity of the model is reduced and the parameters are reduced;
3. The parallel computing mode of attention mechanism accelerates the computing speed.
### 4.3. DISADVANTAGE
1. The Attention mechanism can easily lead to overfitting when the amount of data is too small.
### 4.4. APPLYING
The Seq2Seq model with attention mechanism is suitable for machine translation and image caption text generation tasks.
