# INTRODUCTION
> In machine learning, both classification task and regression task belong to supervised learning. Both of these tasks are essentially using the input independent variables to predict the output value. The difference is that the prediction of the regression task is continuous, while the prediction of the classification task is discrete. In this paper, we will introduce the classical classification and regression models in machine learning.
![输入图片说明](/imgs/2022-12-14/TMB24Vv39rwYIim8.png)

## 1. SVM
### 1.1. CONCEPT
Support vector machine(SVM), the model is to find the hyperplane that maximizes the distance to the closest point in each class. According to different situations, it can be divided into three kinds of support vector machines:
1. Linear Support Vector Machines: Separable case - In the most basic case, a linear decision boundary is built. The edges of the linear classifier are two hyperplanes, and the points on these two hyperplanes are the support vectors.
2. Linear Support Vector Machines: The indivisible case - With soft-margin methods, we learn decision boundaries that allow a certain amount of training error. That is, positive slack variables are introduced on the two marginal hyperplanes.
3. Nonlinear support vector Machine: Kernel technology is used to map the original space to a high dimensional space, so that the originally linear non-separable problem becomes linearly separable. The kernel technique is a method to calculate the similarity in the transformed space using the original set of attributes.
### 1.2. ADVANTAGE
1. The SVM learning problem can be formulated as a convex optimization problem, and known efficient algorithms can be used to find the global minimum of the objective function. However, other classification methods based on greedy learning strategies can only obtain local optimal solutions;
2. SVM can solve nonlinear problems in low-dimensional space through kernel mapping;
3. The decision function of SVM is determined by only a few support vectors, so its complexity depends on the number of support vectors rather than the dimension of the sample space, which avoids the "curse of dimensionality" in a sense;
4. The training samples of SVM are somewhat robust, because adding or reducing non-support vector samples has no effect on the model;
5. The model is interpretable.
### 1.3. DISADVANTAGE
1. SVM is not suitable for large sample size, because it will consume more storage space and operation time;
2. SVM is sensitive to kernel functions.
### 1.4. APPLYING
SVM is suitable for classification problems with small sample size.
```python
import pandas as pd
import numpy as np
from sklearn.svm import SVC
train_df = pd.read_csv('your train file path')
test_df = pd.read_csv('your test file path')
train_array = np.array(train_df)
test_array = np.array(test_df)
labels = np.unique(train_array[:, 0])
transform = {}
for i, l in enumerate(labels):
	transform[l] = i
train = train_array[:, 1:].astype(np.float64)
train_labels = np.vectorize(transform.get)(train_array[:, 0])
test = test_array[:, 1:].astype(np.float64)
test_labels = np.vectorize(transform.get)(test_array[:, 0])
svm = SVC()
svm_grid_search = GridSearchCV(
				svm,{
				'C': [0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000, 10000, np.inf], # penalty factor
				'kernel': ['linear', 'poly', 'rbf', 'sigmod', 'precomputed'], # kernel function
				'degree': [2,3,4], # The dimension of the polynomial poly function, which defaults to 3, is ignored when other kernel functions are selected
				'gamma': ['scale'], # Kernel arguments for 'rbf', 'poly' and 'sigmoid'. The default is' auto ', which selects 1/n_features
				'coef0': [0], # The constant term of the kernel. Useful for 'poly' and 'sigmoid'
				'shrinking': [True], #  Whether to use the shrinking heuristic method, which defaults to true
				'probability': [True], # Whether to use probability estimation. The default value is False to determine whether to enable probability estimation. You need to add this parameter when training the fit() model before you can use the related methods: predict_proba and predict_log_proba
				 'tol': [0.001], # The error value of the stop training is 1e-3 by default
				 'cache_size': [200], # Kernel cache cache size. Default value is 200
				 'class_weight': [None], # The weight of the category is passed in dictionary form. Set class C to weight*C(C in C-SVC)
				 'verbose': [False], # Allows the output of each iteration to be printed
				 'max_iter': [10000000], # Maximum number of iterations. -1 indicates no limit.
				 'decision_function_shape': ['ovr'], # 'ovo', 'ovr' or None, default=None. ovo is a one-to-one classifier, K*(k-1)/2 classifiers are built for K categories, and ovr is a pair of others, K classifiers are built for K categories
				 'random_state': [None] # Random seed
				 },
				 cv = 10, # cross validation
				 n_jobs = 5			
				)
svm_grid_search.fit(train,train_labels)
svm_clf = svm_grid_search.best_estimator_
acc = svm_clf.score(test,test_labels)
y_score = svm_clf.predict_proba(test)
test_labels_onehot = label_binarize(test_labels, classes=np.arange(train_labels.max() + 1))
auprc = average_precision_score(test_labels_onehot, y_score, average='weighted')
# Draw a PR curve
precision, recall, thresholds = precision_recall_curve(test_labels_onehot.ravel(), y_score.ravel())  # The ravel() function pulls the array dimensions into a one-dimensional array 
plt.figure()  
plt.step(recall, precision, where='post')  
plt.xlabel('Recall')  
plt.ylabel('Precision')  
plt.ylim([0.0, 1.05])  
plt.xlim([0.0, 1.0])  
plt.title('Average precision score, micro-averaged over all classes: AP={0:0.2f}'.format(auprc))
```
## 2. Logistic regression
### 2.1. CONCEPT
Logistic regression is a generalized linear regression, but logistic regression assumes that the dependent variable y follows a Bernoulli distribution, while linear regression assumes that the dependent variable y follows a Gaussian distribution. logistic regression can solve binary classification problems by converting the output into a range of 0-1 using the logistic function. For multiclass classification problems, a common approach for logistic regression is to treat one class as a positive class and all other classes as negative classes, thus implementing multinomial logistic regression.
### 2.2. ADVANTAGE
1. The training is fast and the computational complexity mainly depends on the dimension of the input features;
2. Takes up less storage space;
3. The model is interpretable and easy to implement;
4. It can solve both binary and multi-class classification problems.
### 2.3. DISADVANTAGE
1. High-dimensional features can affect the performance of logistic regression because they are prone to multicollinearity problems;
2. Prone to overfitting;
3. Unable to solve nonlinear problems, nonlinear features need to be transformed.
### 2.4. APPLYING
Logistic regression is suitable for data sets with low-dimensional feature space and no multicollinearity problem, which is suitable for solving linear classification problems.
```python
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
pipe = Pipeline(  
    [('sd', StandardScaler()),  
     ('lr', LogisticRegression(random_state=0, max_iter=1000000))])
lr_grid_search = GridSearchCV(  
		    pipe,  
		    { #'lr_random_state': [0],  
		      #'max_iter': [10, 30, 50, 100],     
		      # 'lr__multi_class': ['ovr', 'multinominal'],     'lr__multi_class': ['ovr'],  
		     # 'lr__multi_class': ['ovr'],  
		     'lr__solver': ['liblinear', 'sag', 'saga']  
    },  
     cv=10 
	)
lr_grid_search.fit(features, y)
lr_clf = lr_grid_search.best_estimator_
```
## 3. KNN
### 3.1. CONCEPT
We enter a new sample whose class is the class of the K nearest samples. You can use different distance measures.For example, Euclidean distance, Manhattan distance for numeric data, and Hamming distance for categorical data.

KNN suffers from the "curse of dimensionality", where points in a high-dimensional space do not appear to be close to each other at all. Therefore, before using KNN, it is best to reduce the dimensionality; And since KNN classifies based on distance metric, it also needs to be standardized.
### 3.2. ADVANTAGE
1. Simple and easy to understand;
2. Fast training speed;
3. Insensitive to outliers;
4. Can be used for nonlinear classification problems.
### 3.3. DISADVANTAGE
1. When encountering high-dimensional feature space, the computational complexity is large;
2. If there is a class imbalance problem, the prediction accuracy of rare categories is low;
3. Slower prediction speed compared to logistic regression models.
### 3.4. APPLYING
KNN is suitable for training sets with relatively large sample size and balanced classes.
```python
from sklearn.neighbors import KNeighborsClassifier
pipe = Pipeline(  
    [  
        ('sd', StandardScaler()),  
        ('knn', KNeighborsClassifier())  
    ]  
)  
knn_grid_search = GridSearchCV(  
			    pipe,  
			    param_grid={  
		        'knn__n_neighbors': [i for i in range(1, 6)],  
		        'knn__weights': ['uniform', 'distance'],  
		        'knn__p': [i for i in range(1, 4)],  
		        'knn__leaf_size': [25, 30, 35],  
		        'knn__algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute']  
			    },  
			    cv=10
				)
knn_grid_search.fit(features, y)
knn_clf = knn_grid_search.best_estimator_
```
## 4. Naive bayes
### 4.1. CONCEPT
The naive Bayes classifier is a probabilistic model and makes use of the Bayes principle. The joint probability distribution of input and output categories can be obtained according to the input training set, and the posterior probability of each class of the new sample is calculated. The class with the largest posterior probability is the class of the new sample.
### 4.2. ADVANTAGE
1. Stable classification efficiency;
2. Performs well on small datasets and can handle multi-class classification tasks;
3. Insensitive to missing values
### 4.3. DISADVANTAGE
1. Naive Bayes doesn't work well on data where the features are not independent because it assumes that each feature is independent;
2. Assuming a different prior distribution may affect the prediction performance of naive Bayes;
3. Sensitive to the representation of the input data
### 4.4. APPLYING
Naive Bayes is suitable for small-scale data sets, and there is no significant correlation between input features.
```python
from sklearn.naive_bayes import GaussianNB
naiby = GaussianNB()
naiby_grid_search = GridSearchCV(  
			    naiby,  
			    param_grid={
			    'var_smoothing': [1e-8, 1e-9, 1e-10]
				},  
			    cv=10
)
naiby_grid_search.fit(features, y)
naiby_clf = naiby_grid_search.best_estimator_
```
## 5. Decision tree
### 5.1. CONCEPT
Decision tree is a top-down process that classifies sample data in tree form according to different partitioning algorithms. Where each internal node represents a feature or attribute, and the leaf nodes represent a category. Starting from the root node, the samples are divided into different child nodes, and then further divided according to the characteristics of the child nodes until all the samples are classified into a certain category. There are three partition algorithms for constructing classification decision tree: ID3, C4.5 and CART. The decision tree for regression is split at a location that minimizes the sum of squared errors of the split subsets.
### 5.2. ADVANTAGE
1. The model is simple and intuitive;
2. Good interpretability;
3. Can handle both numeric and categorical data;
4. The requirement of data preprocessing is not high;
5. Insensitive to missing values;
6. Efficient, once the tree is built, the computational complexity of subsequent predictions does not exceed the depth of the tree.
### 5.3. DISADVANTAGE
1. If you do not prune, the model probably overfits;
2. The generated decision tree is unstable and may not return the global optimal decision tree, which is an NP-hard problem;
Decision trees may miss correlations between features.
### 5.4. APPLYING
Decision trees are suitable for datasets with uncorrelated features and small sample sizes.
```python
from sklearn.tree import DecisionTreeClassifier
DTC = DecisionTreeClassifier()
dtc_grid_search = GridSearchCV(  
    DTC,  
    param_grid={
    'criterion': ['gini', 'entropy'],
    'splitter': ['best', 'random'],
    'max_depth': [10, 20, 30, 40, 50, 60]
    },  
    cv=10 
)
dtc_grid_search.fit(features, y)
dtc_clf = dtc_grid_search.best_estimator_
```
## 6. Random forest
### 6.1. CONCEPT
In order to avoid overfitting when training with decision trees, random forest is proposed, which builds multiple decision trees and votes to decide how to classify the input. Random forests use the idea of bagging, which means that if you have N data and B samples with replacement, you will train each decision tree with a different sample, so that the results will generalize. However, the way random forests and bagging select samples is slightly different in two ways:
1. Random Forest selects as many samples as the number of input samples (a sample may be selected multiple times, which may cause some samples not to be selected), while Bagging generally selects fewer samples than the number of input samples.
2. bagging uses all features to obtain a classifier, while Rand Forest selects a part of all features to train a classifier. In general, Random forest works better than Bagging.
### 6.2. ADVANTAGE
1. Can handle high-dimensional data and does not require feature selection since random forests pick features randomly during training;
2. Feature importance can be obtained;
3. Strong generalization ability of the model;
4. Random forests don't need to split the cross-validation set separately;
5. Because the trees are independent during training, the training speed is fast;
6. For imbalanced datasets, random forests can balance the error;
7. Insensitive to missing values;
8. Overfitting can be reduced, but not eliminated.
### 6.3. DISADVANTAGE
1. Features/attributes with more values may have a greater impact on the random forest;
2. Using random forests for classification and regression on noisy datasets can lead to overfitting.
### 6.4. APPLYING
Random forests can be used to deal with high-dimensional, imbalanced data sets, and although they can also solve regression problems, they are more suitable for classification problems.
```python
from sklearn.ensemble import RandomForestClassifier
rfc = RandomForestClassifier()
rfc_grid_search = GridSearchCV(
				rfc,
				param_grid = 		
				{"max_depth": [None],  
                 "max_features": [1, 3, 5],  
                 "min_samples_split": [2, 3, 10],  
                 "min_samples_leaf": [1, 3, 10],  
                 "bootstrap": [False],  
                 "n_estimators": [100, 200, 300, 350],  
                 "criterion": ["gini"]  
                 }
                 cv = 10
				)
rfc_grid_search.fit(features, y)
rfc_clf = rfc_grid_search.best_estimator_
```
## 7. Adaboost
### 7.1. CONCEPT
Adaboost chooses decision tree as the base classifier, and based on the idea of Boosting, the weight of correctly classified samples is reduced in each sampling, and the weight of incorrectly classified samples is increased or kept unchanged. The base classifiers are trained iteratively.
### 7.2. ADVANTAGE
1. Different classifiers can be selected as weak classifiers to improve the accuracy of the model;
2. The weights are determined based on the error rate of the weak classifier to further improve the accuracy of the model.
### 7.3. DISADVANTAGE
1. Class imbalance will reduce the accuracy of the model;
2. Sensitive to outliers.
### 7.4. APPLYING
Adaboost is suitable for datasets with balanced labels and few outliers.
```python
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import AdaBoostClassifier
DTC = DecisionTreeClassifier()
adab = AdaBoostClassifier(DTC, random_state=7)
adab_grid_search = GridSearchCV(
				adab,
				param_grid={
				'base_estimator__criterion': ['gini', 'entropy'],
				'base_estimator__splitter': ['best', 'random'],
				'algorithm': ['SAMME', 'SAMME.R'],
				'max_features':['sqrt','auto','log2'],
				'n_estimators': [1, 2, 3, 10, 20, 30, 40],
				'learning_rate': [0.0001, 0.001, 0.1, 0.2, 0.3, 0.5, 1]
				},
				cv = 10
				)
adab_grid_search.fit(features, y)
adab_clif = adab_grid_search.best_estimator_
```
## 8. GBDT
### 8.1. CONCEPT
GBDT is a gradient boosted decision tree that makes use of the idea of Boosting. The new weak classifier is trained according to the negative gradient information of the current model loss function, and then the trained weak classifiers are combined into the existing model in the form of accumulation. GBDT usually adopts CART as the weak classifier.
### 8.2. ADVANTAGE
1. The prediction phase is fast because it can be computed in parallel between different decision trees;
2. Good generalization and expression ability on densely distributed data sets;
3. Good interpretability and robustness;
4. It can automatically discover higher-order relationships between features and reduce data preprocessing steps such as normalization.
### 8.3. DISADVANTAGE
1. GBDT performs worse than support vector machines or neural networks on high-dimensional and sparse datasets;
2. When GBDT is used to deal with text classification features, its advantage over other models is not as obvious as it is used to deal with numerical features;
3. The training process requires serial training, which reduces the training speed.
### 8.4. APPLYING
GBDT is suitable for densely distributed data sets without categorical features.
```python
import ensemble
gbdt = ensemble.GradientBoostingClassifier()
gbdt_grid_search = GridSearchCV(
				gbdt,
				param_grid={
				'learning_rate': [0.01, 0.05, 0.1, 0.2],
				'max_depth': [3, 6, 8],
				'max_features': ['auto', 'sqrt', 'log2']
				},
				cv = 10
				)
gbdt_grid_search.fit(features, y)
gbdt_clf = gbdt_grid_search.best_estimator_
```
## 9. XGBoost
### 9.1. CONCEPT
XGBoost is different from GBDT in that it adds a regularization term in the decision tree construction stage to control the complexity of the model. The second order Taylor expansion of the cost function was used, and both the first and second order derivatives were used. It supports multiple base classifiers. Instead of using all the data for each training session, we sample the data.
### 9.2. ADVANTAGE
1. The regularization term added by XGBoost helps prevent overfitting, thereby improving the generalization ability of the model;
2. Support multiple base classifiers to improve the prediction accuracy of the model;
3. The sampled data is used in each training, which reduces the calculation time;
4. XGBoost uses parallel computing to select the best segmentation point for each feature, which reduces the computing time;
5. Automatically learn a strategy for dealing with missing values.
### 9.3. DISADVANTAGE
1. Since XGBoost pre-sorts the features of nodes before iteration and traits to select the optimal split point, it consumes a lot of time and storage space when the input is high-dimensional features;
2. Although XGBoost uses parallel computing to select the best split point for each feature, many nodes have low split gain and continue splitting brings unnecessary overhead.
### 9.4. APPLYING
XGBoost is suitable for low and medium dimensional datasets.
```python
from xgboost import XGBClassifier
xgb = XGBClassifier()
xgb_grid_search = GridSearchCV(
				xgb,
				param_grid={
				'booster': ['gbtree'],
				'gamma': [0],
				'max_depth': [5, 6, 7, 8],
				'reg_alpha': [0.1],
				'reg_lambda': [0.1],
				'min_child_weight': [3, 5, 7, 9],
				'learning_rate': [0.1, 0.2, 0.3, 0.5, 0.7]
				},
				cv = 10
				)
xgb_grid_search.fit(features, y)
xgb_clf = xgb_grid_search.best_estimator_
```
## 10. LightGBM
### 10.1. CONCEPT
LightGBM is different from XGBoost in that it is a Histogram-based decision tree algorithm, that is, it does not traverse the sample but the Histogram; The Unilateral Gradient sampling (GOSS) method was used. EFB was used to bind many mutually exclusive features into one feature to achieve dimensionality reduction. A grow-by-leaf strategy with depth constraints is used, meaning that the tree grows vertically rather than horizontally (adding leaves rather than levels of the tree).
### 10.2. ADVANTAGE
1. The Histogram strategy is used to reduce the time complexity;
2. LightGBM uses GOSS strategy to reduce the overhead of time and space;
3. The strategy of growing by leaves also reduces the overhead of time and space;
4. LightGBM's EFB strategy can achieve dimensionality reduction;
5. Support efficient parallelism;
6. You can use categorical features directly;
7. Optimized Cache hit ratio.
### 10.3. DISADVANTAGE
1. LightGBM is a bias-based algorithm, which is sensitive to noisy points；
2. Find the best solution based on the best split variable instead of all features.
### 10.4. APPLYING
It is suitable for large data sets with category features.
```python
from lightgbm.sklearn import LGBMClassifier
lgbm = LGBMClassifier()
lgbm_grid_search = GridSearchCV(
				lgbm,
				param_grid={
				'boosting_type': ['gbdt', 'rf', 'dart', 'goss'],
				'learning_rate': [0.05, 0.1, 0.2],
				'n_estimators': [100, 200, 300, 400],
				'max_depth': [3, 4, 5, 6]
				},
				cv = 10
				)
lgbm_grid_search.fit(features, y)
lgbm_clf = lgbm_grid_search.best_estimator_		
```
## 11. Catboost
### 11.1. CONCEPT
Compared with other gradient boosting algorithms, Catboost puts the processing of categorical features in the training stage rather than the preprocessing stage, and uses a special way to deal with categorical features, that is, the average value of Y labels corresponding to the same category feature is used as the value of the secondary category. Combined features are used; And to prevent overfitting, for each sample, Catboost trains a model that does not update the gradient with the current sample, and uses this model to evaluate the gradient of the current sample.
### 11.2. ADVANTAGE
1. Reduces the overfitting problem;
2. Better processing of categorical features;
3. Training speed increases.
### 11.3. DISADVANTAGE
1. Catboost does not perform well when the input features have order.
### 11.4. APPLYING
It is suitable for unordered data sets with category characteristics.
```python
from catboost import CatBoostClassifier
catb = CatBoostClassifier()
catb_grid_search = GridSearchCV(
				catb,
				param_grid={
				'depth': [6],
				'eval_metric': ['WKappa'],
				'task_type':['CPU'],
				'learning_rate': [0.1, 0.3],
				'l2_leaf_reg': [1,4,9],
				'iterations': [300, 400],
				'random_seed': [42],
				'logging_level':['Silent'],
				'early_stopping_rounds':[500],
				'loss_function': ['MultiClass']
				},
				cv = 10
				)
catb_grid_search.fit(features, y)	
catb_clf = catb_grid_search.best_estimator_
```
## 12. Linear regression
### 12.1. CONCEPT
Linear regression includes univariate linear regression and multiple linear regression. The univariate regression model assumes that there is only one independent variable x and one dependent variable y in the model, and there is a linear relationship between them. The error terms are assumed to be normally distributed random variables with mean 0 and variance σ^2 and are independent of each other. Unary linear regression uses the least squares method to fit the best regression line that minimizes the sum of squared errors. Based on the assumption of univariate linear regression, multiple linear regression also adds the assumption that each independent variable is independent of each other, and describes the linear relationship between multiple independent variables and a dependent variable.
### 12.2. ADVANTAGE
1. Simple and intuitive, easy to implement;
2. Strong interpretability;
3. Each coefficient has its corresponding practical significance.
### 12.3. DISADVANTAGE
1. Linear regression models fail to capture nonlinear relationships in the data;
2. Sensitive to outliers;
3. For multiple linear regression models, multicollinearity between variables will greatly affect the accuracy of the model.
### 12.4. APPLYING
Linear regression models are suitable for small data sets where only linear relationships exist and each feature is independent of the other.
```python
from sklearn.linear_model import LinearRegression
import matplotlib.pyplot as plt

liner = LinearRegression(fit_intercept=True, normalize=False,copy_X=True, n_jobs=1)
liner.fit(x,y)
print(liner)
y_pred = liner.predict(x)

plt.figure(figsize=(5,5))
plt.scatter(x,y)
plt.plot(x,y_pred, color="g")
plt.show()
print(liner.coef_)
print(liner.intercept_)
```
## 13. Polynomial regression
### 13.1. CONCEPT
Polynomial regression is further divided into univariate polynomial regression and multivariate polynomial regression, which can capture nonlinear relationships in variables. Of course, polynomial regression can be solved by multiple linear regression models after appropriate transformations of the independent variables.
### 13.2. ADVANTAGE
1. Strong interpretability;
2. Depict the nonlinear relationship between variables by curves.
### 13.3. DISADVANTAGE
1. Sensitive to outliers;
2. It is easy to overfit if the index is not chosen properly.
### 13.4. APPLYING
Polynomial regression is suitable for solving datasets where nonlinear relationships exist and there are no outliers or few outliers.
## 14. Ridge regression
### 14.1. CONCEPT
Ridge regression uses an improved least squares method to solve the best fit, and adds a penalty term in the error term, that is, adds a λ penalty factor to each parameter. This method is more in line with the actual situation at the cost of losing part of the information and reducing the accuracy.
### 14.2. ADVANTAGE
1. Adding a regularization term reduces overfitting;
2. Reduces the effect of multicollinearity between variables.
### 14.3. DISADVANTAGE
An inappropriate λ may cause the model to underfit and thus decrease the accuracy of the model.
### 14.4. APPLYING
Ridge regression is suitable for data sets with some multicollinearity.
```python
from sklearn.linear_model import Ridge
alphas = [0.1, 0.2, 0.5, 1, 2, 5, 10, 20, 50, 100, 200, 500, 1000]
valid_results = []
for alpha in alphas:
	lr = Ridge(alpha=alpha).fit(train_features, train_y)
	valid_pred = lr.predict(valid_features)
	score = np.sqrt(((valid_pred - valid_y) ** 2).mean()) + np.abs(valid_pred - valid_y).mean()
	valid_results.append(score)
best_alpha = alphas[np.argmin(valid_results)]
lr = Ridge(alpha=best_alpha)
lr.fit(train_features, train_y)
```
## 15. Lasso regression
### 15.1. CONCEPT
The Lasso regression is similar to ridge regression, except that the Lasso adds a regularization term in a different way: it adds a penalty term proportional to the sum of the absolute values of the parameters, whereas ridge regression adds an error term proportional to the sum of the squared parameters.
### 15.2. ADVANTAGE
1. Lasso is more efficient than ridge regression because the L1 norm is more likely to produce sparse solutions than the L2 norm;
2. Reduces the effect of multicollinearity between variables.
### 15.3. DISADVANTAGE
1. An inappropriate λ may cause the model to underfit and thus decrease the accuracy of the model.
### 15.4. APPLYING
The use cases for Lasso and ridge are similar, and for high-dimensional variables, Lasso may perform better than ridge.
```python
from sklearn.linear_model import Ridge,RidgeCV
from sklearn.metrics import mean_squared_error

Lambdas = np.logspace(-5, 2, 200) # construct different lambda
ridge_cofficients = [] 
for Lambda in Lambdas:
	ridge = Ridge(alpha = Lambda, normalize=True)
	ridge.fit(X_train, y_train)
	ridge_cofficients.append(ridge.coef_)
# use RidgeCV to determin best lambda
ridge_cv = RidgeCV(alphas = Lambdas, normalize=True, scoring='neg_mean_squared_error', cv = 10)
ridge_cv.fit(X_train, y_train)
ridge_best_Lambda = ridge_cv.alpha_ # return best lambda

ridge = Ridge(alpha = ridge_best_Lambda, normalize=True)
ridge.fit(X_train, y_train)
pd.Series(index = ['Intercept'] + X_train.columns.tolist(), data = [ridge.intercept_] + ridge.coef_.tolist()) # return Ridge regression coefficients

ridge_predict = ridge.predict(X_test)
RMSE = np.sqrt(mean_squared_error(y_test,ridge_predict))
```
