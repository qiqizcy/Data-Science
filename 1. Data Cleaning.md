# INTRODUCTION
> As we know, data cleaning includes three aspects: filling in missing values, elimination of outliers and identification of noise. This article I list several methods of these aspects respectively. In each method, I will illustrate it's conception and code.

# 1. Missing value processing
## 1.1. Reason
>Objective reason: The mistakes in data collection, data transmission and data storage cause data missing.
> Subjective reason: People's subjective errors, historical limitations and concealing on purpose cause data missing.
## 1.2. Missing mechanism
> * Completely random missing 
> * Random missing
> * Non random missing
## 1.3. Processing content
> * Avoid missing data information as much as possible.
> * Ensure the rationality of filling missing values.
## 1.4. Method
> Notice: 
> If the data were missing randomly, a simple set of solutions would suffice. However, when data are missing in a systematic way, you must determine the impact of missing data on the results.
> 
> Before you choose method to process missing values, first you must consider in advance if observations or variables containing missing data be excluded from the entire analysis or parts of it.
### 1.4.1. Dropping
* Delete sample data which have missing values.
* Delete feature which missing value ratio is larger than your setted threshold.
```python
# delete row
df.drop('a') # a is index
# delete colomn
df.drop('one',axis = 1) # one is column name
df.drop(['one','two'],axis = columns) # delete multiple columns
```
### 1.4.2. Imputation 
> Notice: 
> We don not recommend filling manually and treating missing attribute values as special values, because these methods will cause unaccurate model result.

1. **Statistic Imputation **
**CONCEPT**
For numerical variables, if an outlier is found according to the value distribution of the variable, the median or mode can be selected to fill the missing value; If there are no outliers according to the value distribution of the variable, the mean value can be selected to fill the missing value.(Note that the mean value also can take the mean of the sample points near the missing values.)

	For categorical variables, you can only choose the median or mode to fill the missing values.
**ADVANTAGE**
Simple and calculate easily.
**DISADVANTAGE**
The missing value of interpolation is not as accurate as the other methods.
**APPLYING**
This method is suitable for data sets with a small proportion of missing variable values.
```python
from sklearn.impute import SimpleImputer

# mean value
df_mean = SimpleImputer(missing_values=np.nan, strategy='mean',copy=False)

# meadian
df_median = SimpleImputer(missing_values=np.nan, strategy='median',copy=False)

# mode
df_most_frequent = SimpleImputer(missing_values=np.nan, strategy='most_frequent',copy=False)

# fit_transform
df_A = df.loc[:,'A'].values.reshape(-1,1)
df.loc[:,'A']=df_mean.fit_transform(df_A)
df_B = df.loc[:,'B'].values.reshape(-1,1)
df.loc[:,'B']=df_median.fit_transform(df_B)
df_D = df.loc[:,'C'].values.reshape(-1,1)
df.loc[:,'C']=df_most_frequent.fit_transform(df_C)
```
2. **Regression Imputation**
**CONCEPT**
For categorical variables, you can use model prediction to estimate the missing values by treating the variables with missing values as unknown variables and the other variables as features of the input. Such as logistic regression and random forests.

	For numerical variables, if there's clear linear relationships in the data, you can use linear regression model to predict the missing value.
**ADVANTAGE**
Using model predictions to fill in the missing values is more accurate.
**DISADVANTAGE**
It requires more computational resources and time than the previous approach, and may overfit if linear regression is used.
**APPLYING**
It is suitable for datasets with a large proportion of missing values and variables with missing values are important to the model.
> Regression model and logistic model code will be presented in other files.
3. **KNN Imputation**
**CONCEPT**
You can find k samples that are close to the one with the missing value. The missing values can be replaced with the values of these k nearest neighbors, depending on the data type and the distance metric we choose. KNN belongs to hot deck imputation, which is finding an object most similar to it in the complete data and filling the current value with the most similar value.

	For numeric variables, you can use Euclidean distance, Manhattan distance and cosine distance.

	For categorical variables, you can choose the Hamming distance.
**ADVANTAGE**
This method is more reliable than median and mode imputation.
**DISADVANTAGE**
When the data set is large, KNN will perform a global search when looking for neighbors, which consumes a long time; Since there is little difference between the nearest and farthest neighbors for high-dimentional data sets, the accuracy wil be reduced.
**APPLYING**
KNN is suitable when the data set is small and the feature dimension is not very high.
```python
from sklearn.impute import KNNImputer

imputer = KNNImputer(missing_values=nan, n_neighbors=k, weights='uniform', metric='nan_euclidean', copy=True, add_indicator=False)

# missing_values=nan : all 'nan' will be estimated
# weights='uniform' : The weight of all points in each neighbor are equal.
# metric is the distance metric that searching for neighbors, and you can also define it by yourself.
```
4. **Multiple Imputation**
**CONCEPT**
Multiple imputation method considers that missing values are randomly generated. According to the observed values, multiple sets of comlete data sets are generated by Monte Carlo methd, then each data set is analyzed, then the missing values are filled by the mimimun error of multiple imputation data sets. Both numeric variables and categorical variables can use this method.
**ADVANTAGE**
The bias that may be caused by simple single imputation is avoided.
**DISADVANTAGE**
Execution is not simple, and different estimates will be produced with different experients reducing the reproducibility of experients.
**APPLYING**
This method is suitable for data sets which have random missing values.
```r
imputed_data <- mice::mice(data, m = 25, method = "pmm",
maxit = 10, seed = 12345, print = FALSE)
-- look complete dataset
complete(imputed_data, action = "long", include = TRUE)
```
> For time series, the first thing you need to do is to determine whether the time series has a trend or seasonal component.
> 
> If there are neither trend nor seasonal component, statistical interpolation or random sample interpolation methods can be used;
> 
> If there is a trend component but not a seasonal component, you can use linear interpolation(this is similar to linear regression);
> 
> If both are present, seasonally adjusted interpolation can be used.

5.  **Seasonal Interpolation**
**CONCEPT**
In particular, you can use differencing or fitting method to remove the trend from a time series firstly, then use observation or differencing method to determine the length of the season s, lastly, filling the missing values with value at position t-s. 
```python
import numpy as np
from pandas import read_csv
import matplotlib.pyplot as plt
from statsmodels.tsa.seasonal import seasonal_decompose
from pylab import rcParams

# time series decompose
ts = read_csv('your path')
result = seasonal_decompose(np.array(ts), model='multiplicative', freq=4)

# plot time series decompositions
rcParams['figure.figsize'] = 10, 5
result.plot()
pyplot.figure(figsize=(40,10))
pyplot.show()

# difference method
ts_ = ts.diff(1)

# model fitting method
lreg = LinearReggression()
lreg.fit(x,y)
coef = round(lreg.coef_[0][0],4)
intercept = round(lreg.intercept_[0],4)
pred = lreg.predict(x)
ts_ = ts-pred

# assume you already know Seasonal factors length and the fifteenth point is missing
ts[14] = ts_[14-s]
```
# 2. Outliers processing 
## 2.1. Definition
Autliers means some sample points that are deviate significantly from other points according  to the data distribution.
## 2.2. Criterion of discrimination
1. Simply consider the maximun and minimun values of a variable to be outliers.
2. 3σ rule: Assume that the variable values follow a normal distribution, and the sample points falling outside of plus or minus three standard deviations are considered as outliers.
3. Box plots: Plot the values of the variation, and outliers are considered if the points are more than three standard deviations away from the median.
4. MAD(median absolute deviation): According to the sum of the distances between the observations of a sample point and its mean, if the sum is greater than a certain threshold, it can be considered as an outlier.
5. Density-based: If the local density of a sample point is significantly lower than most of its neighbors, it can be considered as an outlier.
6. Cluster-based: Using a clustering method such as KNN, the clusters that are significantly far away from other clusters and have small sample points can be considered outliers.
## 2.3. Method
### 2.3.1. Dropping
If the number of outliers is very small and their removal will not affect the model too much, you can choose to delete the outliers.
### 2.3.2. Replacement
Replacement of outliers can also be done using methods which are already used to fill missing values. For example, statistical imputation, regression imputation, multiple imputation and so on. There is a new method that only used to replace outliers--the cap method.
1. The cap method
**CONCEPT**
Replace the values at the points above 99% and below 1% with the values at 99% and 1% respectively.
**ADVANTAGE**
Simple and easy to understand.
**DISADVANTAGE**
Some data information may be lost and thus resulting in inaccurate models.
**APPLYING**
This method is suitable for variables that follow a normal distribution.
```python
def blk(floor, root):
	def f(x):
		if x < floor:
			x = floor
		elif x > root:
			x = root
		return x 
	return f
data["col"] = data["col"].map(blk(floor=data["col"].quantitle(0.1), root=data["col"].quantile(0.99)))
```
# 3. Noise processing
## 3.1. Definition
Noisy data in data set ususlly manifests as random fluctuations of the variable around the true value, which can also be understood as random errors or variances in the variable.
## 3.2. The difference between noise and outliers
The observations consist two parts, the true values and the noises, and the outliers belong to the observations, which may be generated due to the true value or the noise.
## 3.3. Bining method
### 3.3.1. Definition
Bining is the process of smoothing the values stored by examining the neighbors(the values around them), using the bin depth to indicate that different bins have the same amout of data,  and the bin width to indicate the range of values in each bin.
### 3.3.2. Bining steps and methods
1. **How to bin?**
* Equal depth bin method -- uniform weight
Each bin contains the same number of records, which is called the depth of the bin.
* Equal width bin method -- uniform interval
The interval ranges of each bin are equal, that is, the variables are equally distributed over the intervals of the attribute values.
* Custom binning - Custom ranges
User-defined interval selection strategy.
* Minimum entropy method
Minimize the uncertainty within each bin.

2. **How to smooth the data in each bin?**
* Mean smoothing
Take the average of the data in the same bin, that is, replace all the data in the bin with the average of the bin.
* Boundary value smoothing method
Replace all the data in the bin except the other boundary by the boundary value with the smaller distance in each bin.
* Median smoothing
Median the data in the same bin, that is, replace all the data in the bin with the median of the item.

```python
# Equal depth division method
import numpy as np
import pandas as pd
import math
x.sort()
depth = x.reshape(int(x.size/3),3) # depth = 3

# Mean smoothing method
# initialize mean_value
mean_depth = np.full([int(x.size/3),3],0)
for i in range(0,int(x.size/3)):
	for j in range(0,3):
		mean_depth[i][j] = int(depth[i].mean())

# mid-value smoothing method
mid_depth = np.full([int(x.size/3),3],0)
for i in range(0,int(x.size/3)):
	for j in range(0,3):
		mid_depth[i][j] = depth[i][int(depth[i].median())]

# Boundary smoothing method
# define boundary for each row
edgeleft = np.range(int(x.size/3))
edgeright = np.range(int(x.size/3))

# initialize edge_depth
edge_depth = np.full([int(x.size/3),3],0)
for i in range(0,int(x.size/3)):
	edgeleft[i]=depth[i][0]
	edgeright[i]=depth[i][-1]
	for j in range(0,3):
		if j == 0:
			edge_depth[i][j]=depth[i][0]
		elif j == 2:
			edge_depth[i][j]=depth[i][2]
		else:
			if math.pow(edgeleft[i]-depth[i][j],2)>math.pow(edgeright[i]-depth[i][j],2):
				edge_depth[i][j] = edgeright[i]
			else:
				edge_depth[i][j] = edgeleft[i]

# Equal width box division method
x.sort()

# define equal interval
bins = [60,70,80,90,100]

# Left closed right open interval [)
x_cuts = pd.cut(x,bins,right=False)

# the number of each interval
number = pd.value_counts(x_cuts)
rows = number.max()

# initialize Equal width box
widthlist = np.full([len(bins)-1,rows],0)
size = x.size

# division
i = 0
for j in range(0,len(bins)-1):
	for k in range(0,number[j]):
		widthlist[j][k] = x[i]
		i += 1
```
