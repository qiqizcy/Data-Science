# INTRODUCTION
>**Why we need to reduce the dimensions of features?**
>  Sometimes we think more data can achieve better model, but are the present dimensions of features real?  For example, when the data has multicollinearity, the real dimensions of the data may smaller than the present dimensions.  Next, just considering the influence of single variable to the target variable may ignore the latent relation between variables. Last, high dimensions of features may increase the complexity of our model. To solve these problems, we need to reduce the feature dimensions before training model.

> **What does the article do?**
> I collect 15 methods of feature dimension reduction, and divide them to two catagories, One is called DROP which means the dimension reduction technique is to drop some variables directly, another is called GENERATE which means we use machine learning model to generate new variables that have smaller dimensions than the original data.
> To make these methods more structured, I list four aspects for every method. Such as **concept or procedure**, **python code**, **advantages and disadvantages** and **applying situations**.
>  
| DROP | GENERATE  |
|--|--|
| Missing Value Ratio | Random Forest |
| Low Variance Filter | Factor Analysis | 
| High Correlation Filter | Principal Component Analysis | 
| Backward Feature Elimination | Independent Component Analysis | 
| Forward Feature Selection | Equidistant Feature Mapping |
|  | t-SNE |
|  | Linear Discriminant Analysis |
|  | Multidimensional Scaling |
|  | Uniform Manifold Approximation Projection|
|  | Self-organizing map | 
# 1. Method1:DROP 
## 1.1. Missing Value Ratio
**CONCEPT**  

If a variable (feature) has too many missing values and we can not fill most of the missing values, this means the information of the variable is too little to use for building model. We can set a threshold, if the missing value ratio of a variable is larger than the threshold,  the variable should be deleted.  

**ADVANTAGE**
 1. Achieve simply.
 2. Reduce the complexity of processing the missing value.
 
**DISADVANTAGE**
 1. We may delete falsely some variables which have important information.

**APPLYING SITUATION**  

This method suit for variables of data which have too many missing values.

```python 
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
data = pd.read_csv('/your filename/datasetname.csv')
missing_value_ratio = data.isnull().sum()/len(data) * 100
threshold = 15% # you can justify it according your datasets
original_variables = data.columns
new_variables = []
for i in range(len(original_variables)):
	if missing_value_ratio[i] <= threshold:
		new_variables.append(original_variables[i])
new_data = data[new_variables]
```
## 1.2. Low Variance Filter
**CONCEPT**  

If the variance of a variable is very small, this means the information carried by the variable is very little, thus the variable should be deleted.  

**ADVANTAGE**

 1. Achieve simply.

**DISADVANTAGE**
 1. Variance is influneced by the range of data, so small range variable may has lower variance than large range vairable. So before applying this method, you need to standardize the data.

**APPLYING SITUATION**  

This method is apply to data which are differentiated by variance.(Same as PCA)

```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
data = pd.read_csv('/your filename/datasetname.csv')
data.dtypes() # check the variable types
# replace the missing value
data['a'].fillna(data['a'].mean,inplace=True)
data['b'].fillna(data['b'].median.inplace=True)
data['c'].fillna(data['c'].mode()[0],inplace=True)
# if the missing value are filled with variable's statistical magnitude
data.isnull().sum()/len(data)*100
# caculate variance
var = data.var(skip_na = True, numeric_only = False)
original_variables = data.columns
new_variables = []
threshold = 13 # you can justify it, and I set threshold equal to 13%
for i in range(0,len(var)):
	if var[i] >= threshold
		new_variables.append(original_variables[i])
data = data[new_variables]
```
## 1.3. High Correlation filter
**CONCEPT**

If high correlation between two variables, this means the two variables probably have a same trend or carry similar information. This situation often occur in linear regression task and when high correlation occur in mutiple variables we call this multicollinearity. And we set a threshold to delete the variable which has a high correlation with another variable.  

**ADVANTAGE**

 1. Achieve simply.
 2. This method can solve the multicollinearity problem.

**DISADVANTAGE**
 
 1. A unsuitable threshold would influence the model performance, sometimes we think high correlation is larger than 0.8, but in real problem, you need to justify the threshold according to correlation matrix.

**APPLYING SITUATION**  

Obviously, this method suits for linear regression model and logistc regression model which probably encounter multicollinearity problem.

```python
import pandas as pd
data = pd.read_csv('/your filename/datasetname.csv')
corr_mtr = data.corr()
threshold = 0.7 # you can justify it.Ususlly if correlation is larger than 0.8, we consider there exists high corrlation
data.drop('a',axis=1,inplace=True) # drop column
```
## 1.4. Backward Feature Elimination
**PROCEDURE**
 
 1. use all variables to build a model
 2. evaluate the model
 3. retrain the model after delete one variable and evaluate the new model
 4. delete the variable if the performance of the model changes very little
 5. repeat (4) (5)

**ADVANTAGE**

 1. This method considers all variables when training a model, and retains the important variable.

**DISADVANTAGE**

 1. The computational expense is expensive and training process is time consuming.
 2. Sometimes we drop a variable by our rules, but actually this variable is working well with other variables.

**APPLYING SITUATION**  

You can use this method when you build a linear regression model or a Logistic regression model. And the dataset can not be very large.

```python
import pandas as pd
from sklearn.linear_model import LinearRegression
from sklearn.feature_selection import RFE
from sklearn import datasets
data = pd.read_csv('/your filename/datasetname.csv')
feature = data.iloc[:,1:]
label = data.iloc[:,1]
lire = LinearRegression()
lore = LogisticRegression()
rfe1 = RFE(estimator = lire, n_features_to_select = 10) # you can choose the number of retained features
rfe1 = rfe1.fit_transform(feature, label)
rfe2 = RFE(estimator = lire, n_features_to_select = 10)
rfe2 = rfe2.fit_transform(feature, label)
```
## 1.5. Forward Feature Selection
**PROCEDURE**
 
 1. use every variable to build model respectively
 2. chose the variable of  best model as the initial variable
 3. add a variable to the model and reevaluate the performance of the model
 4. keep the variable if the performance of the model improve a lot
 5. repeat 4,5 until the performance not improve

**ADVANTAGE**
 
 1. Every variable effect can be observed when add it into the model respectively.

**DISADVANTAGE**

 1. The computational expense is expensive and training process is time consuming.

**APPLYING SITUATION**  

You can use this method when you build a linear regression model or a Logistic regression model. And the dataset can not be very large.

```python
import pandas as pd
from sklearn.feature_selection import f_regression
from sklearn.feature_selection import f_classif
data = pd.read_csv('/your filename/datasetname.csv')
feature = data.iloc[:,1:]
label = data.iloc[:,1]
frg = f_regression(feature,label)
fcl = f_classif(feature,label)
new_variable = []
alpha = 0.05
# regression task
for i in range(0,len(data.columns)-1):
	if frg[1][i] < alpha: # if p-value is smaller than 0.05,we can think there is a correlation between the variable and label. And you can justify alpha
		new_variable.append(data.columns[i])
# classification task
for i in range(0,len(data.columns)-1):
	if fcl[1][i] < alpha: # if p-value is smaller than alpha, we can think there the variable do a big work for distinguish different labels.
		new_variable.append(data.columns[i])
data = data[new_variable]
```
> Tips:
> To understand f_classif, we can use variance analysis, the H0 hypothesis is the variable value can not influence the label value, so if we get a big F-value or a very small p-value, we can reject H0 hypothesis.
> To understand f_regression, we can use correlation coefficient, and the following formula is a statistical magnitude to test the correlation between two variables. If the p-value is almost close to zero, we can think a high correlation between this variable and label.
> ![(1)](/imgs/2022-11-24/C1fRk575RwusUYvI.png) 
## 1.6. Random Forest
**CONCEPT**

Random Forest usually used in feature selection, and it calculate the importance of features automaticlly. We can drop a variable or some variables which importance are not very large.  

**ADVANTAGE**

 1. Fast training speed.
 2. Calculate feature importance automaticly.

**DISADVANTAGE**

 1. When the model input large number of features, it probably overfit.

**APPLYING SITUATION**  

This method are ususlly used in many task, especially classification and regression field.

```python
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.ensemble import RandomForestRegressor
data = pd.read_csv('/your filename/datasetname.csv')
feature = data.iloc[:,1:]
label = data.iloc[:,1]
model = RandomForestRegressor(random_state=1, max_depth=10)
# notice: RandomForestRegressor only supports numeric featrue, so you need to preprocess the categorical feature.
feature['a'] = pd.get_dummies(featrue['a'])
model.fit(feature,label)
features = feature.columns
importances = model.feature_importances_
indices = np.argsort(importances[0:9])  # top 10 features,you can justify it
plt.title('Feature Importances')
plt.barh(range(len(indices)), importances[indices], color='g', align='center')
plt.yticks(range(len(indices)), [features[i] for i in indices])
plt.xlabel('Relative Importance')
plt.show()
# you can choose top features manually,and you can also use SelectFromModel()
from sklearn.feature_selection import SelectFromModel
feature = SelectFromModel(model)
Fit = feature.fit_transform(df, train.Item_Outlet_Sales)
```

# Method2. GENERATE
## 2.1. Factor Analysis
**CONCEPT**

Factor analysis is a common statistical method, which can extract the common factor from multiple variables and get the optimal solution. In the factor analysis, we will group variables according to the correlation, variables within a particular group of correlation is high, correlation between groups is low. We call each group a factor, it's the combination of multiple variables. Compared with original datasets, these factors have a smaller number and not loss the information.  

**PROCEDURE**

 1. standardization
 2. calculate correlation coefficient matrix C
 3. calculate the characteristic value and characteristic vector of the C
 4. determine the number of factors
 5. construct initial factor load matrix A
 6. build factor model
 7. fit a rotation transformation on the initial factor loading matrix A to make the  matrix structure more simple.  If the initial factors are not uncorrelated, you can use orthogonal rotation with maximum variance(方差极大正交旋转); If the initial factors are correlated, you can oblique rotation(斜交旋转).
 8. denote factors as linear combination
 9. caculate factor score

**ADAVANTAGE**

 1. This method can solve high correlation in variables problem.
 2. Improve the interpretability in variables by rotation factor load matrix.

**DIADVANTAGE**

 1. It uses least square method to calculate the factor score, and sometime this method not work well.

**APPLYING SITUATION**  

This method suits for dataset which has high correlative variables.

```python
import numpy as np
import pandas as pd
from sklearn import preprocessing
import numpy.linalg as nlg
from math import sqrt
from numpy import eye, asarray, dot, sum, diag
from numpy.linalg import svd
data = pd.read_csv('/your filename/datasetname.csv')
# standardization
zscore = preprocessing.StandardScaler()
data_zs = zscore.fit_transform(data)
# calculate correlation coefficient matrix C
C = data_zs.corr()
# calculate the characteristic value and characteristic vector of the C
eig_value,eig_vector=nlg.eig(C) 
eig = pd.DataFrame()
eig['names'] = data.columns
eig['eig_value'] = eig_value
threshold = 0.8 # you can justify it,it means the least explanary degree
for k in range(1,len(data.columns)+1):
	if eig['eig_value'][:k].sum()/eig['eig_value'].sum() >= threshold:
	print(k)
	break
eig['eig_value'][:k].sum()/eig['eig_value'].sum() # the explanary degree of these factors
# construct initial factor load matrix A
col0=list(sqrt(eig_value[0])*eig_vector[:,0]) #factor load matrix 1th column
col1=list(sqrt(eig_value[1])*eig_vector[:,1]) 
...
colk=list(sqrt(eig_value[k])*eig_vector[:,k]) #factor load matrix kth column
A=pd.DataFrame([col0,col1,...,colk]).T #构造因子载荷矩阵A
A.columns=['factor1','factor2',...,'factork'] #因子载荷矩阵A的公共因子
# build factor model
h=np.zeros(10) # 10 is the number of features,and h is variable common degree,reflect the dependence of the variables for common factors, if h is closer to 1, this means the explanary degree of factors is strong, the effect of factor analysis is good.
D=np.mat(np.eye(10)) # specific factor variance or the variance contribution of the factor, reflect the variable contribution of the common factors, measure the relative importance of factors
A=np.mat(A)
for i in range(10):
	a = A[i,:] * A[i,:].T
	h[i] = a[0,0]
	D[i,i] = 1-a[0,0]
# rotation transformation and linear combination
def varimax(Phi,gamma = 1.0, q =10, tol = 1e-6): # define Maximum rotation function of variance
	p,k = Phi.shape
	R = eye(k)
	d=0
	for i in range(q):
        d_old = d
        Lambda = dot(Phi, R) 
        u,s,vh = svd(dot(Phi.T,asarray(Lambda)**3 - (gamma/p) * dot(Lambda, diag(diag(dot(Lambda.T,Lambda)))))) # SVD singular value decomposition
        R = dot(u,vh) # construct orthogonal matrix
        d = sum(s) # sum(singular value)
    if d_old!=0 and d/d_old:
        return dot(Phi, R) # return rotation matrix
rotation_mat=varimax(A) 
rotation_mat=pd.DataFrame(rotation_mat)
# calculate factor score
data=np.mat(data) # Matrix processing
factor_score=(data).dot(A) 
factor_score=pd.DataFrame(factor_score)
factor_score.columns=['factorA','factorB',..,'factorK'] # rename
```
> You can use FactorAnalysis library which is wraped in sklearn
```python
from sklearn.decomposition import FactorAnalysis
FA = FactorAnalysis(n_components = k).fit_transform(df[feat_cols].values) 
# you can select the number of factors according to factor variance contribution which is calculated in the above code
```
## 2.2. Principal Component Analysis(PCA) 
**CONCEPT**

PCA is to transform a group of correlated variables into mutually **uncorrelated variables** through orthogonal transformation, and new variables are the **linear combination** of original variables. The first principal component has the max variance value. The other principal components try to explain remaining variance.  

**ADVANTAGE**

 1. PCA uses less new variables to reflect most original information.
 2. PCA focus on comprehensive evaluation of information contribution and influence.

**DISADVANTAGE**

 1. Sometimes it's hard to explain the symbol significance of factor loading.
 2. PCA just consider maximizing the direction of the variance and sometimes may cause data distribution more complex.

**APPLYING SITUATION**  

PCA suits for linear problem and numeric variables. And it also suits data that satisfies a Gaussian distribution. PCA is usually used for unsupervised task.

```python
import pandas as pd
from sklearn.decomposition import PCA
data = pd.read_csv('/your filename/datasetname.csv')
feature = data.iloc[:,1:]
pca = PCA(n_components = 4) # you can justify the n_components according to pca.explained_variance_ratio_
pca_result = pca.fit_transform(features.values)
plt.plot(range(4), pca.explained_variance_ratio_)
plt.plot(range(4), np.cumsum(pca.explained_variance_ratio_))
plt.title("Component-wise and Cumulative Explained Variance")
```
## 2.3. Independent Component Analysis(ICA)
**CONCEPT**

Compared with PCA, ICA is aimed to transform a group of correlated variables into mutually **independent variables**, rather than use variances of different directions to decide the principal components, it decides principal components according the dispersion degree of data in the direction. And ICA thinks all the components have same importance.  

**ADVANTAGE**

 1. ICA can generate variables that are indendent of each other.
 2. Gaussian distribution is not necessary.

**DISADVANTAGE**

 1. Sometimes the data may not have enough variables to satisfy the condition of ICA.

**APPLYING SITUATION**  

Compared with PCA, ICA is more suitable for data which not satisfy Gaussian distribution. 

```python
import pandas as pd
from sklearn.decomposition import FastICA 
data = pd.read_csv('/your filename/datasetname.csv')
feature = data.iloc[:,1:]
ICA = FastICA(n_components=3, random_state=12) # 
X=ICA.fit_transform(feature.values)
```
## 2.4. Linear Discriminant Analysis(LDA)
**CONCEPT**

Compared with PCA, LDA is projecting raw data into a low-dimensional space, and try to make the same labels of data aggregation and different labels of data dispersed. It's a supervised learning method and it has some assumptions on data distribution: each kind of data is satisfy gaussian distribution; each kind of data has same covariance.

 **ADVANTAGE**

 1. Fast calculating speed.
 2. Take full advantage of prior knowledge(label).

**DISADVANTAGE**

 1. If the number of feature is larger than the data size, the performance of LDA will be worse.
 2. In practice, the assumptions of LDA may not be fully satisfied.

**APPLYING SITUATION**  

This method suits for data that satisfies a Gaussian distribution. (Same as PCA) And LDA applys mainly in classification task. LDA is usually used for supervised task.

```python
import pandas as pd
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
data = pd.read_csv('/your filename/datasetname.csv')
feature = data.iloc[:,1:]
label = data.iloc[:,1]
lda = LinearDiscriminantAnalysis(n_components=2)
lda.fit(feature,label)
X_new = lda.transform(feature)
plt.scatter(X_new[:, 0], X_new[:, 1],marker='o',c=y)
plt.show()
```
## 2.5. Multidimensional Scaling(MDS)
**CONCEPT**  

MDS is aimed to project the points in a higher-dimensional coordinate into a lower-dimensional space, and keeping the similarity between the points as constant as possible.

```python
import numpy as np
def MDS(D,d):
	D = np.asarray(D)
	DSquare = D ** 2
	totalMean = np.mean(DSquare)
	columnMean = np.mean(DSquare, axis = 0)
	rowMean = np.mean(DSquare, axis = 1)
	B = np.zeros(DSquare.shape)
	for i in range(B.shape[0]):
		for j in range(B.shape[1]):
			B[i][j] = -0.5 * (DSquare[i][j] - rowMean[i] - columnMean[j] + totalMean)
	eigVal, eigVec = np.linalg.eig(B) # calculate characteristic value and characteristc vector
	eigValSorted_indices = np.argsort(eigVal) # get the sorted index of characteristic value
	topd_eigVec = eigVec[:,eigValSorted_indices[:-d-1:-1]]
	X = np.dot(topd_eigVec, np.sqrt(np.diag(eigVal[:-d-1:-1])))
	return X
```
## 2.6. Self-organizing map (SOM)
**CONCEPT**

Self-organizing map is an unsupervised neural network method, rather than usual artificial neutal network using backpropagation algorithm to update weights, it uses a leaning strategy called competitive learning mapping the input data to a lower dimention and discrete space which can keep the topology of the input data.

**ADVANTAGE**

1. Compared with k-means method, SOM does not need a initial value k.(But SOM needs initial weights)
2. SOM has good visualization effect.
3. SOM is an unsupervised method, therefore it does not need to discriminate training set and testing set. 
4. SOM can keep the topology of the input data.

**DISADVANTAGE**

1. The accuracy of SOM is not very high, because it also update it's neighbor after update a weight.
2. If the initial weights do not reflect original distribution well, it will occur some "died nodes" which are never updated.

```python
import pandas as pd
from minisom import MiniSom
data = pd.read_excel('your path')
som = MiniSom(x,y,input_len,sigma=1.0,learning_rate=0.5,neighborhood_function='gaussian')
# x: competition layer x dimention
# y: competition layer y dimention
# input_len: input layer dimention
# sigma: The initial radius of the winning neighborhood
# neighborhood_function: How the winning neighborhood is calculated, such as “gaussian”， "bubble"， "mexican_hat"，'triangle'
som.train(data,100) # trains the SOM with 100 iterations
```

> You can also use SOM for clustering, high dimensional visualization, data compression, classification, and of course, feature extraction.
## 2.7. Equidistant Feature Mapping (ISOMAP)
**CONCEPT**  

Compared with MDS, ISOMAP replaces euclidean distance with shortest path distance(use manifold learning) to fit manifold data better.

```python
import pandas as pd
from sklearn import manifold 
import matplotlib.pyplot as plt
data = pd.read_csv('/your filename/datasetname.csv')
feature = data.iloc[:,1:]
label = data.iloc[:,1]
trans_data = manifold.Isomap(n_neighbors=5, n_components=3, n_jobs=-1).fit_transform(feature.values)
# n_neighbors: the number of neighbors for every points
# n_components: the number of manifold coordinations 
# n_jobs = -1: cpu cores
plt.figure(figsize=(12,8))
plt.title('Decomposition using ISOMAP')
plt.scatter(trans_data[:,0], trans_data[:,1])
plt.scatter(trans_data[:,1], trans_data[:,2])
plt.scatter(trans_data[:,2], trans_data[:,0])
```
## 2.8. t-SNE
**CONCEPT**  

Compared with PCA, t-SNE is not a linear dimension reduction technique, on the contrary, it's a nonlinear technique. And it projects n dimensions to 2 or 3 dimensions capturing complex manifold structure.

```python
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.manifold import TSNE 
data = pd.read_csv('/your filename/datasetname.csv')
feature = data.iloc[:,1:]
label = data.iloc[:,1]
tsne = TSNE(n_components=3, n_iter=300).fit_transform(feature.values)
plt.figure(figsize=(12,8))
plt.title('t-SNE components')
plt.scatter(tsne[:,0], tsne[:,1])
plt.scatter(tsne[:,1], tsne[:,2])
plt.scatter(tsne[:,2], tsne[:,0])
```
## 2.9. Uniform Manifold Approximation Projection(UMAP)
**CONCEPT**  

UMAP assums data are uniformly distributed in the manifold space, and it can approximate and project data to a lower dimension space.  

**PROCEDURE**
 1. learn the manifold structure from higher dimension space.
 2. denote the manifold structure in a lower dimension.
```python
import pandas as pd
import matplotlib.pyplot as plt
import umap
data = pd.read_csv('/your filename/datasetname.csv')
feature = data.iloc[:,1:]
label = data.iloc[:,1]
umap_data = umap.UMAP(n_neighbors=5, min_dist=0.3, n_components=3).fit_transform(feature.values)
# n_neighbors: the number of neighbors
# min_dist: allowed embeded close degree, the larger value can ensure the distribution of embeded points more uniformed
plt.figure(figsize=(12,8))
plt.title('Decomposition using UMAP')
plt.scatter(umap_data[:,0], umap_data[:,1])
plt.scatter(umap_data[:,1], umap_data[:,2])
plt.scatter(umap_data[:,2], umap_data[:,0])
```
> Although linear model is good to the robustness of noise, its structrue is simple, and has limited ability of expression. 2.5 -2.8 are the methods which suit for non-linear data processing, and t-SNE is more visualized than ISOMAP, UMAP also suits for high dimensions data and faster than t-SNE. ALL the four methods  capture the manifold structure data.
