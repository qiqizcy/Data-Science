{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "274fac60",
   "metadata": {},
   "source": [
    "实例一、京东商品界面的爬取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2dd1ac9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<script>window.location.href='https://passport.jd.com/new/login.aspx?ReturnUrl=http%3A%2F%2Fitem.jd.com%2F100022384184.html'</script>\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "url = 'https://item.jd.com/100022384184.html'\n",
    "try:\n",
    "    r = requests.get(url)\n",
    "    r.raise_for_status()\n",
    "    r.encoding = r.apparent_encoding\n",
    "    print(r.text[:1000])\n",
    "except:\n",
    "    print('爬虫失败')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c799e8",
   "metadata": {},
   "source": [
    "实例二、亚马逊商品界面的爬取（如果User-agent中直接显示python request，网站可能拒绝了爬虫访问（当访问结果显示出现了错误时），需要通过headers参数给User-agent改成浏览器访问）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "34f70c65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\");\n",
      "\n",
      "(function(d,e){function h(f,b){if(!(a.ec>a.mxe)&&f){a.ter.push(f);b=b||{};var c=f.logLevel||b.logLevel;c&&c!==k&&c!==m&&c!==n&&c!==p||a.ec++;c&&c!=k||a.ecf++;b.pageURL=\"\"+(e.location?e.location.href:\"\");b.logLevel=c;b.attribution=f.attribution||b.attribution;a.erl.push({ex:f,info:b})}}function l(a,b,c,e,g){d.ueLogError({m:a,f:b,l:c,c:\"\"+e,err:g,fromOnError:1,args:arguments},g?{attribution:g.attribution,logLevel:g.logLevel}:void 0);return!1}var k=\"FATAL\",m=\"ERROR\",n=\"WARN\",p=\"DOWNGRADED\",a={ec:0,ecf:0,\n",
      "pec:0,ts:0,erl:[],ter:[],mxe:50,startTimer:function(){a.ts++;setInterval(function(){d.ue&&a.pec<a.ec&&d.uex(\"at\");a.pec=a.ec},1E4)}};l.skipTrace=1;h.skipTrace=1;h.isStub=1;d.ueLogError=h;d.ue_err=a;e.onerror=l})(ue_csm,window);\n",
      "\n",
      "ue.stub(ue,\"event\");ue.stub(ue,\"onSushiUnload\");ue.stub(ue,\"onSushiFlush\");\n",
      "\n",
      "var ue_url='/rd/uedata',\n",
      "ue_sid='458-6697580-3808348',\n",
      "ue_mid='AAHKV2X7AFYLW',\n",
      "ue_sn='www.amazon.cn',\n",
      "ue_furl='fls-cn.amazon.cn',\n",
      "ue_surl='https://unagi.amazon.cn/1/events/com.amazon\n"
     ]
    }
   ],
   "source": [
    "#import requests\n",
    "url = 'https://www.amazon.cn/dp/B00FYU60V4/ref=sr_1_2?pf_rd_i=1403206071&pf_rd_m=A1U5RCOVU0NYF2&pf_rd_p=58f1ad31-25b4-4c68-985a-b04112cd68b4&pf_rd_r=XNQBHV3TQ2T2W8C52NJZ&pf_rd_s=merchandised-search-top-4&pf_rd_t=101&qid=1646130623&s=grocery&sr=1-2'\n",
    "try:\n",
    "    kv = {'user-agent':'Mozilla/5.0'}#Mozilla/5.0是一个比较标准的浏览器\n",
    "    r = requests.get(url,headers = kv)\n",
    "    r.raise_for_status()\n",
    "    r.encoding = r.apparent_encoding\n",
    "    print(r.text[1000:2000])\n",
    "except:\n",
    "    print('爬虫失败')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc9adc5",
   "metadata": {},
   "source": [
    "实例三、百度、360搜索关键字提交"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1fef96bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://www.baidu.com/s?wd=python\n",
      "515361\n"
     ]
    }
   ],
   "source": [
    "#import requests\n",
    "keyword = 'python'\n",
    "try:\n",
    "    kv = {'wd':keyword}#360的键值对改成{'p':keyword}\n",
    "    r = requests.get('http://www.baidu.com/s',params = kv)\n",
    "    print(r.request.url)#看一下给百度的request对象的地址是什么\n",
    "    r.raise_for_status()#请求是否通过\n",
    "    print(len(r.text))\n",
    "except:\n",
    "    print('爬取失败')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "649fc675",
   "metadata": {},
   "source": [
    "实例四、网络图片的爬取和存储"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1a723ebb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "文件保存成功\n"
     ]
    }
   ],
   "source": [
    "#import requests\n",
    "import os\n",
    "url = 'https://img.tt98.com/d/file/tt98/20200119152361374/051eec1c4d.jpg'\n",
    "root = 'E://pics//'\n",
    "path = root + url.split('/')[-1]\n",
    "try:\n",
    "    if not os.path.exists(root):\n",
    "        os.mkdir(root)\n",
    "    if not os.path.exists(path):#如果此文件不存在，就要执行下面的代码，将二进制的图片保存为文件\n",
    "        r = requests.get(url)\n",
    "        with open(path,'wb') as f:\n",
    "            f.write(r.content)#content保存的是二进制的内容\n",
    "            f.close()\n",
    "            print('文件保存成功')\n",
    "    else:\n",
    "        print('文件已存在')\n",
    "except:\n",
    "    print('爬取失败')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b238efa",
   "metadata": {},
   "source": [
    "实例五、IP地址归属地的自动查询"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "08c96636",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "爬取失败\n"
     ]
    }
   ],
   "source": [
    "#import requests\n",
    "url = 'http://m.ip138.com/ip.asp?ip='\n",
    "try:\n",
    "    r = requests.get(url+'202.204.80.112')\n",
    "    r.raise_for_status()\n",
    "    r.encoding = r.apparent_encoding\n",
    "    print(r.text[-500:])#一般都要有一个约束范围\n",
    "except:\n",
    "    print('爬取失败')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b45d37d2",
   "metadata": {},
   "source": [
    "失败的原因可能是因为我没有登录"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5c8c9f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "soup = BeautifulSoup('<p>data</p>','html.parser')#前一个参数是需要解析的html格式的信息，后面一个参数是解析器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "12643ab2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<html><head><title>This is a python demo page</title></head>\\r\\n<body>\\r\\n<p class=\"title\"><b>The demo python introduces several python courses.</b></p>\\r\\n<p class=\"course\">Python is a wonderful general-purpose programming language. You can learn Python from novice to professional by tracking the following courses:\\r\\n<a href=\"http://www.icourse163.org/course/BIT-268001\" class=\"py1\" id=\"link1\">Basic Python</a> and <a href=\"http://www.icourse163.org/course/BIT-1001870001\" class=\"py2\" id=\"link2\">Advanced Python</a>.</p>\\r\\n</body></html>'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = requests.get('http://python123.io/ws/demo.html')\n",
    "demo = r.text\n",
    "demo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d5c128",
   "metadata": {},
   "source": [
    "中国大学排名定向爬虫"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ec347b35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    排名    \t 学校名称 \t    总分    \n",
      "    名次    \t 学校名称 \t   所在地区   \n",
      "    1     \t 北京大学 \t    北京    \n",
      "    2     \t 清华大学 \t    北京    \n",
      "    3     \t 复旦大学 \t    上海    \n",
      "    4     \t 浙江大学 \t    浙江    \n",
      "    5     \t 南京大学 \t    江苏    \n",
      "    6     \t上海交通大学\t    上海    \n",
      "    7     \t华中科技大学\t    湖北    \n",
      "    8     \t中国科学技术大学\t    安徽    \n",
      "    9     \t中国人民大学\t    北京    \n",
      "    10    \t 天津大学 \t    天津    \n",
      "    10    \t 武汉大学 \t    湖北    \n",
      "    12    \t 南开大学 \t    天津    \n",
      "    13    \t 山东大学 \t    山东    \n",
      "    14    \t 中山大学 \t    广东    \n",
      "    15    \t西安交通大学\t    陕西    \n",
      "    16    \t哈尔滨工业大学\t   黑龙江    \n",
      "    17    \t 东南大学 \t    江苏    \n",
      "    18    \t 四川大学 \t    四川    \n",
      "    19    \t 吉林大学 \t    吉林    \n"
     ]
    }
   ],
   "source": [
    "#import requests\n",
    "#from bs4 import BeautifulSoup\n",
    "import bs4\n",
    "def getHTMLText(url):\n",
    "    try:\n",
    "        r = requests.get(url,timeout = 30)\n",
    "        r.raise_for_status()\n",
    "        r.encoding = r.apparent_encoding\n",
    "        return r.text\n",
    "    except:\n",
    "        return ''\n",
    "    \n",
    "def fillUnivList(ulist,html):\n",
    "    soup = BeautifulSoup(html,'html.parser')\n",
    "    for tr in soup.find('tbody').children:\n",
    "        if isinstance(tr,bs4.element.Tag):#由于节点与节点之间可能存在一些字符串他们不是tr标签，所以要将其过滤掉\n",
    "            tds = tr('td')#获取'tr'标签中的'td'标签\n",
    "            ulist.append([tds[0].text.strip(),tds[1].text.strip(),tds[2].text.strip()])#把需要的'td'标签提取出来\n",
    "#ulist是一个二维列表，每个子字符串对应一个tr标签，即大学\n",
    "\n",
    "def printUnivList(ulist,num):\n",
    "    print('{:^10}\\t{:^6}\\t{:^10}'.format('排名','学校名称','总分'))\n",
    "    ##优化：\n",
    "    #tplt = \"{0:^10}\\t{1:{3}^10}\\t{2:^10}\"#这里的三指的是format里面的第三个变量，中文字符空格\n",
    "    #print(tplt.format(\"排名\",\"学校名称\",\"总分\",chr(12288)))\n",
    "    for i in range(num):\n",
    "        u = ulist[i]\n",
    "        print('{:^10}\\t{:^6}\\t{:^10}'.format(u[0],u[1],u[2]))\n",
    "    ##优化\n",
    "    #print(tplt.format(u[0],u[1],u[2],chr(12288)))\n",
    "def main():\n",
    "    uinfo = []\n",
    "    url = 'http://www.gaosan.com/gaokao/241219.html'\n",
    "    html = getHTMLText(url)\n",
    "    fillUnivList(uinfo,html)\n",
    "    printUnivList(uinfo,20)#显示20所大学的排名\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f17c82",
   "metadata": {},
   "source": [
    "未经过优化的界面对其效果不好，因为在中文文本的情况，补齐时仍然用的是英文字符，所以下面的优化要更改为中文字符"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ad06aa51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    排名    \t　　　　　学校名称　　　　　　\t    总分    \n",
      "    名次    \t　　　　　学校名称　　　　　　\t   综合得分   \n",
      "    1     \t　　　　　北京大学　　　　　　\t  100.00  \n",
      "    2     \t　　　　　清华大学　　　　　　\t  98.78   \n",
      "    3     \t　　　　　复旦大学　　　　　　\t  82.14   \n",
      "    4     \t　　　　　浙江大学　　　　　　\t  81.98   \n",
      "    5     \t　　　　　南京大学　　　　　　\t  81.43   \n",
      "    6     \t　　　　上海交通大学　　　　　\t  81.34   \n",
      "    7     \t　　　　华中科技大学　　　　　\t  80.49   \n",
      "    8     \t　　　中国科学技术大学　　　　\t  80.44   \n",
      "    9     \t　　　　中国人民大学　　　　　\t  80.41   \n",
      "    10    \t　　　　　天津大学　　　　　　\t  80.38   \n",
      "    10    \t　　　　　武汉大学　　　　　　\t  80.38   \n",
      "    12    \t　　　　　南开大学　　　　　　\t  79.28   \n",
      "    13    \t　　　　　山东大学　　　　　　\t  79.19   \n",
      "    14    \t　　　　　中山大学　　　　　　\t  78.75   \n",
      "    15    \t　　　　西安交通大学　　　　　\t  76.48   \n",
      "    16    \t　　　　哈尔滨工业大学　　　　\t  76.23   \n",
      "    17    \t　　　　　东南大学　　　　　　\t  75.79   \n",
      "    18    \t　　　　　四川大学　　　　　　\t  75.73   \n",
      "    19    \t　　　　　吉林大学　　　　　　\t  75.64   \n"
     ]
    }
   ],
   "source": [
    "#优化\n",
    "\n",
    "def getHTMLText(url):\n",
    "    try:\n",
    "        r = requests.get(url, timeout=30)\n",
    "        r.raise_for_status()\n",
    "        r.encoding = r.apparent_encoding\n",
    "        return r.text\n",
    "    except:\n",
    "        return \"\"\n",
    "\n",
    "def fillUnivList(ulist, html):\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    for tr in soup.find('tbody').children:\n",
    "        if isinstance(tr, bs4.element.Tag):\n",
    "            tds = tr('td')\n",
    "            ulist.append([tds[0].text.strip(), tds[1].text.strip(), tds[3].text.strip()])\n",
    "\n",
    "def printUnivList(ulist, num):\n",
    "    tplt = \"{0:^10}\\t{1:{3}^15}\\t{2:^10}\"\n",
    "    print(tplt.format(\"排名\",\"学校名称\",\"总分\",chr(12288)))\n",
    "    for i in range(num):\n",
    "        u=ulist[i]\n",
    "        print(tplt.format(u[0],u[1],u[2],chr(12288)))\n",
    "    \n",
    "def main():\n",
    "    uinfo = []\n",
    "    url = 'http://www.gaosan.com/gaokao/241219.html'\n",
    "    html = getHTMLText(url)\n",
    "    fillUnivList(uinfo, html)\n",
    "    printUnivList(uinfo, 20) # 20 univs\n",
    "if __name__==\"__main__\":#这里就是程序入口，不加也可以的，这个是说如果直接运行代码，就可以执行，如果是导入进来的，就不运行\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c354e5",
   "metadata": {},
   "source": [
    "出的错误有：\n",
    "1、网页链接不正确\n",
    "2、网页格式已经改变了，要删除的多余空格，用.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc37bbe",
   "metadata": {},
   "source": [
    "淘宝商品信息定向爬虫"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e859482e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "序号  \t价格      \t商品名称            \n",
      "   1\t498.00  \tZHUYIYI 重工纯棉米白色蕾丝连衣裙女夏中长款显瘦仙女泡泡袖长裙\n",
      "   2\t229.99  \t甜美TIMI白色法式连衣裙早春2022新款温柔气质高级感衬衫初恋裙\n",
      "   3\t139.00  \t安小落 白色提花小白裙女夏法式新款赫本风气质收腰中长连衣裙子\n",
      "   4\t189.90  \t乌龙羊/法式碎花裙女夏薄款短袖V领不规则鱼尾裙仙女长款连衣裙\n",
      "   5\t259.00  \t乌龙羊/法式复古吊带连衣裙女春设计感小众长款收腰显瘦内搭裙子\n",
      "   6\t179.00  \tUR2022春季新品女装法系气质方领紧身宝藏连衣裙WL02R7AN2000\n",
      "   7\t429.00  \tunderpass 原创年会裙辣妹风蝴蝶结设计单排扣v领纯欲西装连衣裙\n",
      "   8\t219.99  \t森马连衣裙女雪纺盘扣文艺2022春季新款质感泡泡袖碎花裙清新甜美\n",
      "   9\t319.00  \t范智乔【水墨晕染】吊带连衣裙女春夏小众设计感温柔复古碎花长裙\n",
      "  10\t169.99  \t芭比姨 辣妹蝴蝶结连衣裙早春2022新款女装设计感针织露背短裙\n",
      "  11\t178.00  \tCHACHASTU 法式复古高级感气质连衣裙女碎花春2022新款设计感长裙\n",
      "  12\t399.90  \tONE MORE2022春季新款碎花连衣裙女黑色法式裙子女秋冬修身吊带裙\n",
      "  13\t299.00  \tUR2022夏季新品女装潮酷个性吊带皮裙宝藏连衣裙WG05S7FN2000\n",
      "  14\t148.00  \tFAMOUS◆绿色连衣裙法式复古宫廷长款纯欲温柔茶歇公主仙女裙女夏\n",
      "  15\t599.00  \tONLY2022春季新款层次设计肌理感礼服气质吊带连衣裙女|122207002\n",
      "  16\t148.00  \t法式连衣裙女设计感小众黄色纱裙长裙2021夏新款收腰超仙公主裙子\n",
      "  17\t159.00  \t米米子大码胖mm春款女装2022年新款高级感裙子女春秋牛仔裙连衣裙\n",
      "  18\t69.00   \t法式初恋甜美连衣裙收腰夏2021新款薄款设计感小众雪纺a字长裙女\n",
      "  19\t188.00  \t2022春夏新款早春温柔仙女裙白色法式连衣裙女收腰挂脖长裙连体衣\n",
      "  20\t199.00  \tVEGA CHANG白色气质连衣裙设计感法式小众收腰显瘦衬衫裙女春新款\n",
      "  21\t349.00  \tWooHa/吾哈春季新款裙子修身渐变女团风紧身针织挂脖吊带连衣裙女\n",
      "  22\t119.99  \t芭比姨 露背针织连衣裙女春装新款设计感性感内搭链条v领挂脖短裙\n",
      "  23\t199.00  \t【商场同款】太平鸟收腰连衣裙2021新款学院风夏新款a字裙小黑裙\n",
      "  24\t89.00   \tYT&UR女装V领休闲薄款荷叶牛仔连衣裙2021夏季新款WG13SBNN2002\n",
      "  25\t455.00  \tWooHa/吾哈新款法式复古宫廷长裙民族风印花气质春秋长袖连衣裙女\n",
      "  26\t460.00  \t【XIAOLI设计师系列】乐町连衣裙2022春夏新款公主裙蓬蓬裙女装\n",
      "  27\t268.00  \taaaaxbbb飒姐快冲绿咖色风衣连衣裙\n",
      "  28\t432.00  \t【2.12新品】乐町长袖心动连衣裙2022春夏新款碎花雪纺粉色宽松\n",
      "  29\t369.00  \tMiddletone | 赫本裙 dio*秀款天丝羊毛光泽感收腰吊带连衣裙礼服\n",
      "  30\t79.00   \t性感辣妹风拉链半高领包臀短裙女春秋长袖针织小黑裙紧身连衣裙潮\n",
      "  31\t179.00  \t茶歇在逃公主裙超仙高级感别致漂亮凡尔赛洛丽塔泡泡袖连衣裙子女\n",
      "  32\t329.90  \t太平鸟法式泡泡袖郁金香连衣裙2022春季新款纯欲千金风收腰小黑裙\n",
      "  33\t368.00  \t阔色收腰吊带裙子连衣裙蓬蓬裙女2022新款早春装小黑裙长裙夏预售\n",
      "  34\t115.00  \t迪士尼在逃公主裙法式蕾丝连衣裙露后背蝴蝶结设计感系带仙女裙子\n",
      "  35\t168.00  \tCHACHA春装法式复古温柔风荷叶碎花连衣裙仙女显瘦气质裙子小个子\n",
      "  36\t338.00  \t阔色法式白连衣裙2022新款早春女设计感泡泡袖长裙子衬衫裙夏\n",
      "  37\t78.00   \t法式复古气质超仙长袖白色打底裙2021秋冬新款修身内搭蕾丝连衣裙\n",
      "  38\t219.00  \tFunnJ方几 微醺欲妹 复古碎花吊带连衣裙女 设计感收腰显瘦长裙春\n",
      "  39\t149.00  \tKATHI黑色赫本风抹胸连衣裙2022夏法式小众设计感蓬蓬裙sst短裙\n",
      "  40\t219.99  \t森马连衣裙女垂感雪纺裙森系2022春新款v领木耳边肌理碎花裙浪漫\n",
      "  41\t79.90   \t春季2022年新款大码女装法式赫本风连衣裙子御姐轻熟风气质早春装\n",
      "  42\t189.00  \tSEEKSIMPLE “进击的甜妹” 泡泡袖连衣裙女春秋法式气质绑带裙子\n",
      "  43\t899.00  \tCROLINE卡洛琳2022春季新款蝴蝶结系带花边法式连衣裙ECRCAC03\n",
      "  44\t1995.00 \t[预售][新季]GANNI 2022早春新品女格纹棉质混纺泡泡纱连衣裙NAP\n",
      "  45\t2395.00 \t[预售][新季]GANNI 2022早春新品女嵌花混纺斜纹布迷你连衣裙NAP\n",
      "  46\t1329.00 \tDOUBLOVE贝爱22春夏新款气质通勤西装连衣裙V领甜美泡泡裙摆\n",
      "  47\t128.00  \t橘里CHIU维多利亚赫本法式系带仙女精致温柔全蕾丝连衣裙2022夏季\n",
      "  48\t249.00  \t小镇姗姗柏林甜梦气质优雅法式连衣裙女2022年早春温柔网纱仙女裙\n",
      "  49\t249.00  \t小镇姗姗柏林甜梦气质优雅法式连衣裙女2022年早春温柔网纱仙女裙\n",
      "  50\t79.00   \t艺术生风格奶甜乖裙子法式高级感初恋茶歇在逃公主裙2022春连衣裙\n",
      "  51\t89.00   \t配大衣针织连衣裙2021年秋冬新款法式别致设计感温柔风打底毛衣裙\n",
      "  52\t128.00  \t橘里CHIU清冷温婉印花苎麻仙女中国风改良旗袍气质少女连衣裙修身\n",
      "  53\t98.00   \tMimo dk 法式拼接连衣裙长款原创复古方领收腰泡泡袖长裙夏仙女裙\n",
      "  54\t75.00   \t法式淡黄碎花连衣裙夏女文艺气质收腰显瘦温柔风方领仙女雪纺长裙\n",
      "  55\t58.00   \t法式白色吊带连衣裙女夏季小个子甜美减龄茶歇裙收腰显瘦初恋裙子\n",
      "  56\t158.00  \tvintage古董裙欧式宫廷风洛丽塔在逃公主裙法式洋装小礼服连衣裙\n",
      "  57\t168.00  \t夏季设计感小众高贵优雅气质高级感温柔风复古仙女长款连衣裙子女\n",
      "  58\t299.00  \t乌龙羊/法式黑色长袖连衣裙女春季荷叶边蕾丝长裙v领收腰显瘦裙子\n",
      "  59\t239.90  \t乌龙羊/法式高级感吊带连衣裙女秋季无袖连衣裙气质显瘦长款裙子\n",
      "  60\t139.00  \tUR2022夏季新品女装休闲时尚薄款紧身宝藏连衣裙WU07R7EN2000\n",
      "  61\t299.00  \tUR2022春季新品女装气质减龄叠层宝藏连衣裙WU02R7AN2000\n",
      "  62\t210.00  \t【2.24新品】乐町蓝莓软糖连衣裙2022春新款雪纺碎花法式海岛来信\n",
      "  63\t369.00  \tWooHa/吾哈早春新款法式田园风复古少女甜美百褶裙V领法式连衣裙\n",
      "  64\t199.00  \t【2.12新品】乐町长袖衬衫连衣裙2022春夏新款小黑裙长款女宽松\n",
      "  65\t188.00  \t迪士尼茶歇在逃公主裙超仙甜美高级感法式浪漫一字肩吊带连衣裙春\n",
      "  66\t146.00  \t2022年新款收腰显瘦法式小众气质温柔风泡泡袖桔梗连衣裙裙子女夏\n",
      "  67\t179.00  \t肉完法式针织连衣裙女秋冬小个子2021年新款显瘦打底爆款毛衣裙\n",
      "  68\t88.00   \t奶甜在逃公主裙子茶歇法式仙女长裙女春气质超仙温柔风连衣裙别致\n",
      "  69\t136.90  \t韩国一字肩荷叶边长袖连衣裙女2022春新款设计感两穿V领仙女长裙\n",
      "  70\t288.00  \t2022年新款轻奢名媛小香风套装女春季高级感网红时尚半身裙两件套\n",
      "  71\t88.00   \t奶甜在逃公主裙子法式气质蕾丝长裙女春仙女超仙温柔风连衣裙别致\n",
      "  72\t189.00  \t然而和风日系灰色西装连衣裙女2022年春季新款复古收腰V领背带裙\n",
      "  73\t158.00  \t冬季本命年新年衣服战袍红色针织毛衣连衣裙女小众设计小香风套装\n",
      "  74\t79.00   \t法式小众桔梗茶歇裙子仙女超仙森系温柔风女神V领雪纺连衣裙女夏\n",
      "  75\t79.90   \t2022年新款法式复古初恋早春装雪纺气质设计感小众连衣裙子女装夏\n",
      "  76\t138.00  \tMIUJU复古碎花连衣裙夏季收腰气质法式泡泡袖V领初恋长裙女2021新\n",
      "  77\t1034.00 \tCROLINE卡洛琳2022春季新款OL通勤V领收腰网纱长款连衣裙ECRCAA27\n",
      "  78\t308.00  \t陈太太CHENTAITAI-绿野仙踪- 拼接泡泡袖长款森系连衣裙\n",
      "  79\t118.90  \t慕兔法式温柔泡泡袖碎花连衣裙女2022春季新款收腰显瘦a字中长裙\n",
      "  80\t188.00  \t白色长裙超仙法式高级感春装赫本风纯欲气质惊艳仙女连衣裙子套装\n",
      "  81\t168.00  \t冷淡轻熟风西装连衣裙女中长款春秋A字裙显瘦高级感裙子小众设计\n",
      "  82\t358.00  \t早春装新款赫本风娃娃领绣花长裙小香风高级感泡泡袖呢子连衣裙女\n",
      "  83\t239.00  \t宿本牛仔连衣裙女2022春季新款中长款复古翻领裙子收腰a字裙显瘦\n",
      "  84\t359.00  \t任小艺赫本风复古格子吊带裙长款2022新款设计感小众高腰连衣裙女\n",
      "  85\t179.00  \t午后商店◆自制红色丝绒吊带连衣裙女春季法式复古赫本风内搭裙子\n",
      "  86\t368.00  \tDPLAY2022年春装法式复古收腰显瘦连衣裙鎏光纱高级感红色礼服裙\n",
      "  87\t998.00  \tSHOWROOMPLUS千鸟格抹胸连衣裙女春季长款早春设计感小礼服裙子\n",
      "  88\t138.00  \t早春女装2022年新款盐系穿搭高级感御姐轻熟上衣两件套装连衣裙子\n",
      "  89\t399.00  \tWooHa/吾哈春季新款格子蓬蓬裙荷叶边娃娃领粉色碎花衬衫连衣裙女\n",
      "  90\t349.00  \t黑茉莉与甜酒 奥菲利亚 重工鱼骨法式复古设计感吊带连衣裙长裙夏\n",
      "  91\t69.00   \t茶歇法式小众吊带连衣裙油画风仙气超仙森系初恋收腰裙子别致长裙\n",
      "  92\t79.00   \t早春女装2022年新款盐系穿搭高级感御姐轻熟上衣两件套装连衣裙子\n",
      "  93\t148.00  \tFAMOUS◆新年战袍红色连衣裙女复古蝴蝶结宫廷风公主裙中长款秋冬\n",
      "  94\t188.00  \t法式白色连衣裙女装早春急2022年新款小香风长裙冷淡风高级感气质\n",
      "  95\t338.00  \tDPLAY2021秋定制撞色蝴蝶结装饰连衣裙气质方领小黑裙复古小礼服\n",
      "  96\t50.90   \t奶乖连衣裙子奶甜温柔系别致设计小众2022年新款白色女装大码早春\n",
      "  97\t225.11  \tMQueen复古水钻花朵露腰绑带方领小黑裙提花蓬蓬裙法式连衣裙9069\n",
      "  98\t209.98  \tRUKIA/克莱因蓝连衣裙又a又飒复古拼接法式小众设计牛仔裙高级感\n",
      "  99\t138.00  \t秋冬高级感御姐法式复古赫本风背带裙设计感小众连衣裙子两件套装\n",
      " 100\t168.00  \t170高个子超长裙吊带连衣裙子到脚踝秋冬季气质收腰皮裙长款森系\n",
      " 101\t998.00  \tSHOWROOMPLUS 春季抹胸小众高级感连衣裙女长款收腰高端礼服裙子\n",
      " 102\t58.00   \tddgirl 法式复古吊带碎花连衣裙女春秋新款设计感小众减龄中长裙\n",
      " 103\t88.00   \t法式复古翻领连衣裙女早春季高腰减龄娃娃领设计感气质女神范长裙\n",
      " 104\t49.90   \t蓬蓬裙子夏季女装小个子仙女法式绿色泡泡袖v领连衣裙2022新款夏\n",
      " 105\t89.90   \t2022年早春季新款设计感小礼服白色气质女神范连衣裙收腰显瘦长裙\n",
      " 106\t289.00  \t任小艺法式复古碎花连衣裙女小众设计2022春秋新款高级感收腰长裙\n",
      " 107\t188.00  \t现货原创正版梦境游乐园OP洛丽塔Lolita日常洋装连衣裙公主裙春秋\n",
      " 108\t188.00  \t岛歌夫人 修身针织连衣裙打底裙V领毛衣裙长裙气质收腰显瘦女秋冬\n",
      " 109\t188.00  \t我是你的cc阿 长夜灯火~法式复古春冬娃娃领假两件拼接连衣裙\n",
      " 110\t119.00  \t2022早春新款碎花连衣裙女春秋小众设计法式复古方领蕾丝长款裙子\n",
      " 111\t99.00   \t乌龙羊/法式泡泡袖白色连衣裙女夏薄款休闲收腰长款裙子气质长裙\n",
      " 112\t199.00  \t乌龙羊/ 法式碎花长袖连衣裙女秋季收腰显瘦长款裙子气质方领长裙\n",
      " 113\t369.00  \tUR2022春季新品女装潮酷叠层迷你吊带宝藏连衣裙WV09S7AN2002\n",
      " 114\t388.00  \t乐町拼接绑带连衣裙2022春夏新款拼接裙子女装收腰通勤简约甜美\n",
      " 115\t288.00  \t【2.12新品】乐町拼接撞色长裙2022春夏新款吊带连衣裙纱裙超仙\n",
      " 116\t199.00  \tUR2022春季新品女装复古薄款菱格polo宝藏连衣裙WU07R7AN2000\n",
      " 117\t269.00  \tDPLAY2022年春新款法式小香风V领森林绿粗花呢短袖连衣裙\n",
      " 118\t168.00  \t秋冬法式赫本风惊艳纯欲凡尔赛公主生日party宴会宫廷风连衣裙子\n",
      " 119\t238.00  \t长裙法式赫本风惊艳纯欲凡尔赛公主生日party宴会宫廷风连衣裙子\n",
      " 120\t329.00  \tWooHa/吾哈春季新款辣妹紧身包臀裙腰部V领显瘦黑色针织连衣裙女\n",
      " 121\t59.94   \t法式复古拼接针织连衣裙女赫本风高端优雅气质温柔别致设计感长裙\n",
      " 122\t36.98   \t早春高级感绿色吊带连衣裙女装2022春季新款法式小众别致长裙裙子\n",
      " 123\t89.90   \t温柔风v领荷叶边拼接连衣裙女2021冬季新款韩版气质显瘦时尚裙子\n",
      " 124\t69.90   \t法式泡泡袖白色连衣裙赫本风v领抽绳气质小裙子女夏装a字茶歇裙潮\n",
      " 125\t158.00  \t正版兔子爱丽丝JSK原创Lolita裙洛丽塔洋装甜系可爱吊带连衣裙夏\n",
      " 126\t349.00  \tONLY2022春季新款时尚短款泡泡袖镂空A字牛仔连衣裙女|122242015\n",
      " 127\t68.00   \t泡泡袖连衣裙冷淡风女装高级感长裙法式复古气质修身小裙子女夏装\n",
      " 128\t148.00  \t2022春季新款复古法式少女珍珠拼接蕾丝连衣裙收腰显瘦超仙裙礼服\n",
      " 129\t72.00   \tPPHOME时髦感立马UP~复古牛仔连衣裙女秋季收腰皮带显瘦衬衫短裙\n",
      " 130\t138.00  \t裙子仙女超仙森系海边拍照衣服初恋古着茶歇在逃公主高级感连衣裙\n",
      " 131\t332.99  \tOrange Desire吊带连衣裙女2022春季新款白色方领裙子温柔高级感\n",
      " 132\t59.90   \t春装2022年新款性感纯欲辣妹针织连衣裙子女秋冬季修身包臀小黑裙\n",
      " 133\t169.99  \t芭比姨 吊带连衣裙春装新款女设计感公主蓬蓬裙短裙蝴蝶结西装裙\n",
      " 134\t219.00  \t许大晴 2色~复古气质荷叶边假两件连衣裙春季新款中长款V领显瘦裙\n",
      " 135\t89.00   \t法式配大衣的长裙子白色仙女气质内搭打底蕾丝加绒连衣裙女秋冬季\n",
      " 136\t9999.00 \tnololita幻觉玫瑰· 【截团展示】提花两件套鱼骨连衣裙\n"
     ]
    }
   ],
   "source": [
    "#import requests\n",
    "import re\n",
    "\n",
    "def getHTMLText(url):\n",
    "    try:\n",
    "        kv={'cookie':'_samesite_flag_=true; miid=468378521263974516; v=0; cookie2=19c6916efe532477fd6eb25bc23bc906; _tb_token_=f6e373be657d6; cna=5yviFn0iuhACAbfK0HZ5biyR; thw=cn; hng=CN%7Czh-CN%7CCNY%7C156; t=3fce3263f0886ea70265dbde94c3d100; xlly_s=1; sgcookie=E100R5y9wMOKyVzteyLFPTs4RO71coMnrwEED2lQLNtn7aJmiSxp%2BjDAMjDB48e3MiUsNuMQNlzcSIMA%2FxY4IbOFllQAXth1zZaGINFaVps4BNnkYTa2K1pJN04aaOcDhDBT; unb=3012667424; uc1=cookie14=UoewBjjcSGcDJw%3D%3D&pas=0&existShop=false&cookie15=VFC%2FuZ9ayeYq2g%3D%3D&cookie21=UIHiLt3xThH8t7YQoFNq&cookie16=URm48syIJ1yk0MX2J7mAAEhTuw%3D%3D; uc3=nk2=pZ3Tedcb7Q%3D%3D&id2=UNDVcDl5O9YmwA%3D%3D&lg2=Vq8l%2BKCLz3%2F65A%3D%3D&vt3=F8dCvUCV31XOCBdqTiE%3D; csg=0578ff75; lgc=%5Cu4E03%5Cu4E03lfn; cancelledSubSites=empty; cookie17=UNDVcDl5O9YmwA%3D%3D; dnk=%5Cu4E03%5Cu4E03lfn; skt=0d70846f6ee12c63; existShop=MTY0NjIwNjYyMA%3D%3D; uc4=nk4=0%40pyp1g%2FDoMnnszNHw95DGuPfD&id4=0%40UgclHSSUzUD0MYE51D2%2BwQn0FxbE; tracknick=%5Cu4E03%5Cu4E03lfn; _cc_=W5iHLLyFfA%3D%3D; _l_g_=Ug%3D%3D; sg=n4d; _nk_=%5Cu4E03%5Cu4E03lfn; cookie1=BxuRtU4muyaiYkIU1NfC6Bd2YWqVpo9J9NctorqAHvo%3D; enc=sanO1d%2B%2B%2BrLyjLlgqPHDNk%2BsiQdc1kofFKQGIWT0OgV7%2B20R3emDMu%2Ba6Ggj56b08Jx6S9pzNgpo7JbWrwsQFw%3D%3D; JSESSIONID=71D67A4771011D364BF14B50C7BE17DC; isg=BCQkkz8SGoikTlK7p-H4Nxoo5SIWvUgnIAuokD5FsO-y6cSzZs0Yt1pLrUFxKoB_; l=eBSlH6Gljql1E1IEBOfanurza77OSKRYYuPzaNbMiOCPOF5B5b9cW6DnGx86C3GVh6XDR3ylR_DpBeYBqQAonxv92j-la_kmn; tfstk=cgjVBg0L3oE2jmK_JntaGtKrLzWAwbOMO0JeoZpa1a6hjKf05fpuFfpyQHZBo',\n",
    "                'user-agent':'Mozilla/5.0'}\n",
    "        r = requests.get(url,headers = kv, timeout=30)\n",
    "        r.raise_for_status()\n",
    "        r.encoding = r.apparent_encoding\n",
    "        return r.text\n",
    "    except:\n",
    "        return \"\"\n",
    "    \n",
    "def parsePage(ilt, html):\n",
    "    try:\n",
    "        plt = re.findall(r'\\\"view_price\\\"\\:\\\"[\\d\\.]*\\\"',html)\n",
    "        tlt = re.findall(r'\\\"raw_title\\\"\\:\\\".*?\\\"',html)#最小匹配找到名称\n",
    "        for i in range(len(plt)):\n",
    "            price = eval(plt[i].split(':')[1])\n",
    "            title = eval(tlt[i].split(':')[1])\n",
    "            ilt.append([price , title])\n",
    "    except:\n",
    "        print(\"\")\n",
    "#如果通过正则表达式搜索就可以找到需要的结果，就不需要使用beautifulsoup来解析html文档了\n",
    "\n",
    "def printGoodsList(ilt):\n",
    "    tplt = \"{:4}\\t{:8}\\t{:16}\"\n",
    "    print(tplt.format(\"序号\", \"价格\", \"商品名称\"))\n",
    "    count = 0\n",
    "    for g in ilt:\n",
    "        count = count + 1\n",
    "        print(tplt.format(count, g[0], g[1]))\n",
    "        \n",
    "def main():\n",
    "    goods = '连衣裙'\n",
    "    depth = 3\n",
    "    start_url = 'https://s.taobao.com/search?q=' + goods\n",
    "    infoList = []\n",
    "    for i in range(depth):\n",
    "        try:\n",
    "            url = start_url + '&s=' + str(44*i)#s是通过观察html得知其代表每个界面开始商品的标号（可见能看懂并找到html中的一些规律也很重要）\n",
    "            html = getHTMLText(url)\n",
    "            parsePage(infoList, html)\n",
    "        except:\n",
    "            continue\n",
    "    printGoodsList(infoList)\n",
    "main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e8f2ce",
   "metadata": {},
   "source": [
    "这个案例我出错的原因就是因为淘宝限制所有的爬虫，所以需要模拟一个服务器的操作，我在opera浏览器里面打开源代码，然后检查元素，点击‘NetWork’,再点击‘doc’,然后点击那个search文档，找到cookie和user-agent复制过来，将headers改为复制的内容，即可显示。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd4416d",
   "metadata": {},
   "source": [
    "股票数据定向爬虫"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "d0187623",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CrawBaiduStocksA.py\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import traceback\n",
    "import re\n",
    "import bs4\n",
    "def getHTMLText(url):\n",
    "    try:\n",
    "        r = requests.get(url,timeout=30)\n",
    "        r.raise_for_status()\n",
    "        r.encoding = r.apparent_encoding\n",
    "        return r.text\n",
    "    except:\n",
    "        return \"\"\n",
    "\n",
    "def getStockList(lst, stockURL):#获得股票的信息列表\n",
    "    html = getHTMLText(stockURL)#获得一个页面\n",
    "    soup = BeautifulSoup(html, 'html.parser')#解析页面\n",
    "    a = soup.find_all('a')#股票信息保存在a标签中，要找到a标签\n",
    "    for i in a:#遍历每个a标签\n",
    "        try:\n",
    "            href = i.attrs['href']\n",
    "            lst.append(re.findall(r\"[s][hz]\\d{6}\", href)[0])#判断在href中的链接是否符合正则表达式，并且提取出来\n",
    "        except:\n",
    "            continue#如果链接不是我们要找的，直接continue让程序继续运行\n",
    "\n",
    "def getStockInfo(lst, stockURL, fpath):\n",
    "    for stock in lst:\n",
    "        url = stockURL + stock + \".html\"#百度的链接+个股的链接形成一个访问个股页面的url\n",
    "        html = getHTMLText(url)#获取个股页面的内容\n",
    "        try:\n",
    "            if html==\"\":\n",
    "                continue#若当前页面为空页面则继续执行\n",
    "            infoDict = {}#存储返回的所有个股信息\n",
    "            soup = BeautifulSoup(html, 'html.parser')\n",
    "            stockInfo = soup.find('div',attrs={'class':'stock-bets'})#搜索个股封装的标签信息\n",
    "            if isinstance(stockInfo,bs4.element.Tag):\n",
    "                name = stockInfo.find_all(attrs={'class':'bets-name'})[0]#获取个股名称\n",
    "                infoDict.update({'股票名称': name.text.split()[0]})#将个股名称增加到字典中，用空格分开，并获取其第一部分，得到完整的名称\n",
    "                keyList = stockInfo.find_all('dt')#获取键的列表\n",
    "                valueList = stockInfo.find_all('dd')#获取值的列表\n",
    "            #将列表还原为键值对（字典）\n",
    "                for i in range(len(keyList)):\n",
    "                    key = keyList[i].text\n",
    "                    val = valueList[i].text\n",
    "                    infoDict[key] = val\n",
    "            #写入文件\n",
    "                with open(fpath, 'a', encoding='utf-8') as f:\n",
    "                    f.write(str(infoDict) + '\\n')\n",
    "        except:\n",
    "            traceback.print_exc()#获得出现异常的错误信息\n",
    "            continue\n",
    "\n",
    "def main():\n",
    "    stock_list_url = 'http://quote.eastmoney.com/stock_list.html#sh'#获得股票列表的链接\n",
    "    stock_info_url = 'http://so.cfi.cn/so.aspx?txquery='#获取股票信息的链接的主体部分\n",
    "    output_file = 'E:/GuChengStockInfo.txt'\n",
    "    slist=[]\n",
    "    getStockList(slist, stock_list_url)#获得股票列表\n",
    "    getStockInfo(slist, stock_info_url, output_file)#根据股票列表到相关网站获取股票信息并将其存在文件中\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01422de6",
   "metadata": {},
   "source": [
    "还是没有成功"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "0420232e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "当前进度: 100.00%"
     ]
    }
   ],
   "source": [
    "#CrawBaiduStocksB.py\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import traceback\n",
    "import re\n",
    "\n",
    "def getHTMLText(url, code=\"utf-8\"):#实现写好编码，因为apparent encoding要分析完全文，所以比较费时\n",
    "    try:\n",
    "        r = requests.get(url)\n",
    "        r.raise_for_status()\n",
    "        r.encoding = code\n",
    "        return r.text\n",
    "    except:\n",
    "        return \"\"\n",
    "\n",
    "def getStockList(lst, stockURL):\n",
    "    html = getHTMLText(stockURL, \"GB2312\")\n",
    "    soup = BeautifulSoup(html, 'html.parser') \n",
    "    a = soup.find_all('a')\n",
    "    for i in a:\n",
    "        try:\n",
    "            href = i.attrs['href']\n",
    "            lst.append(re.findall(r\"[s][hz]\\d{6}\", href)[0])\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "def getStockInfo(lst, stockURL, fpath):\n",
    "    count = 0\n",
    "    for stock in lst:\n",
    "        url = stockURL + stock + \".html\"\n",
    "        html = getHTMLText(url)\n",
    "        try:\n",
    "            if html==\"\":\n",
    "                continue\n",
    "            infoDict = {}\n",
    "            soup = BeautifulSoup(html, 'html.parser')\n",
    "            stockInfo = soup.find('div',attrs={'class':'stock-bets'})\n",
    "\n",
    "            name = stockInfo.find_all(attrs={'class':'bets-name'})[0]\n",
    "            infoDict.update({'股票名称': name.text.split()[0]})\n",
    "            \n",
    "            keyList = stockInfo.find_all('dt')\n",
    "            valueList = stockInfo.find_all('dd')\n",
    "            for i in range(len(keyList)):\n",
    "                key = keyList[i].text\n",
    "                val = valueList[i].text\n",
    "                infoDict[key] = val\n",
    "            \n",
    "            with open(fpath, 'a', encoding='utf-8') as f:\n",
    "                f.write( str(infoDict) + '\\n' )\n",
    "                count = count + 1\n",
    "                print(\"\\r当前进度: {:.2f}%\".format(count*100/len(lst)),end=\"\")#动态的显示打印进度，其中\\r可以在同一行，每次从头开始\n",
    "        except:\n",
    "            count = count + 1\n",
    "            print(\"\\r当前进度: {:.2f}%\".format(count*100/len(lst)),end=\"\")\n",
    "            continue\n",
    "\n",
    "def main():\n",
    "    stock_list_url = 'http://quote.eastmoney.com/stock_list.html#sh'\n",
    "    stock_info_url = 'http://so.cfi.cn/so.aspx?txquery='\n",
    "    output_file = 'E:/BaiduStockInfo.txt'\n",
    "    slist=[]\n",
    "    getStockList(slist, stock_list_url)\n",
    "    getStockInfo(slist, stock_info_url, output_file)\n",
    "\n",
    "main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f816bf",
   "metadata": {},
   "source": [
    "看了很多博客仍然未找到解决办法......."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "d4a6159f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scrapy\n",
      "  Downloading Scrapy-2.6.1-py2.py3-none-any.whl (264 kB)\n",
      "Requirement already satisfied: pyOpenSSL>=16.2.0 in f:\\anaconda\\lib\\site-packages (from scrapy) (20.0.1)\n",
      "Requirement already satisfied: zope.interface>=4.1.3 in f:\\anaconda\\lib\\site-packages (from scrapy) (5.3.0)\n",
      "Collecting itemloaders>=1.0.1\n",
      "  Downloading itemloaders-1.0.4-py3-none-any.whl (11 kB)\n",
      "Collecting parsel>=1.5.0\n",
      "  Downloading parsel-1.6.0-py2.py3-none-any.whl (13 kB)\n",
      "Requirement already satisfied: setuptools in f:\\anaconda\\lib\\site-packages (from scrapy) (52.0.0.post20210125)\n",
      "Collecting PyDispatcher>=2.0.5\n",
      "  Downloading PyDispatcher-2.0.5.zip (47 kB)\n",
      "Collecting queuelib>=1.4.2\n",
      "  Downloading queuelib-1.6.2-py2.py3-none-any.whl (13 kB)\n",
      "Requirement already satisfied: lxml>=3.5.0 in f:\\anaconda\\lib\\site-packages (from scrapy) (4.6.3)\n",
      "Collecting protego>=0.1.15\n",
      "  Downloading Protego-0.2.1-py2.py3-none-any.whl (8.2 kB)\n",
      "Requirement already satisfied: cryptography>=2.0 in f:\\anaconda\\lib\\site-packages (from scrapy) (3.4.7)\n",
      "Collecting w3lib>=1.17.0\n",
      "  Downloading w3lib-1.22.0-py2.py3-none-any.whl (20 kB)\n",
      "Collecting itemadapter>=0.1.0\n",
      "  Downloading itemadapter-0.4.0-py3-none-any.whl (10 kB)\n",
      "Collecting tldextract\n",
      "  Downloading tldextract-3.2.0-py3-none-any.whl (87 kB)\n",
      "Collecting cssselect>=0.9.1\n",
      "  Downloading cssselect-1.1.0-py2.py3-none-any.whl (16 kB)\n",
      "Collecting Twisted>=17.9.0\n",
      "  Downloading Twisted-22.1.0-py3-none-any.whl (3.1 MB)\n",
      "Collecting service-identity>=16.0.0\n",
      "  Downloading service_identity-21.1.0-py2.py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: cffi>=1.12 in f:\\anaconda\\lib\\site-packages (from cryptography>=2.0->scrapy) (1.14.5)\n",
      "Requirement already satisfied: pycparser in f:\\anaconda\\lib\\site-packages (from cffi>=1.12->cryptography>=2.0->scrapy) (2.20)\n",
      "Collecting jmespath>=0.9.5\n",
      "  Downloading jmespath-0.10.0-py2.py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: six>=1.6.0 in f:\\anaconda\\lib\\site-packages (from parsel>=1.5.0->scrapy) (1.15.0)\n",
      "Requirement already satisfied: pyasn1-modules in f:\\anaconda\\lib\\site-packages (from service-identity>=16.0.0->scrapy) (0.2.8)\n",
      "Requirement already satisfied: pyasn1 in f:\\anaconda\\lib\\site-packages (from service-identity>=16.0.0->scrapy) (0.4.8)\n",
      "Requirement already satisfied: attrs>=19.1.0 in f:\\anaconda\\lib\\site-packages (from service-identity>=16.0.0->scrapy) (20.3.0)\n",
      "Collecting twisted-iocpsupport<2,>=1.0.2\n",
      "  Downloading twisted_iocpsupport-1.0.2-cp38-cp38-win_amd64.whl (45 kB)\n",
      "Collecting constantly>=15.1\n",
      "  Downloading constantly-15.1.0-py2.py3-none-any.whl (7.9 kB)\n",
      "Collecting Automat>=0.8.0\n",
      "  Downloading Automat-20.2.0-py2.py3-none-any.whl (31 kB)\n",
      "Collecting incremental>=21.3.0\n",
      "  Downloading incremental-21.3.0-py2.py3-none-any.whl (15 kB)\n",
      "Collecting hyperlink>=17.1.1\n",
      "  Downloading hyperlink-21.0.0-py2.py3-none-any.whl (74 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.6.5 in f:\\anaconda\\lib\\site-packages (from Twisted>=17.9.0->scrapy) (3.7.4.3)\n",
      "Requirement already satisfied: idna>=2.5 in f:\\anaconda\\lib\\site-packages (from hyperlink>=17.1.1->Twisted>=17.9.0->scrapy) (2.10)\n",
      "Requirement already satisfied: requests>=2.1.0 in f:\\anaconda\\lib\\site-packages (from tldextract->scrapy) (2.25.1)\n",
      "Collecting requests-file>=1.4\n",
      "  Downloading requests_file-1.5.1-py2.py3-none-any.whl (3.7 kB)\n",
      "Requirement already satisfied: filelock>=3.0.8 in f:\\anaconda\\lib\\site-packages (from tldextract->scrapy) (3.0.12)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in f:\\anaconda\\lib\\site-packages (from requests>=2.1.0->tldextract->scrapy) (2020.12.5)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in f:\\anaconda\\lib\\site-packages (from requests>=2.1.0->tldextract->scrapy) (1.26.4)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in f:\\anaconda\\lib\\site-packages (from requests>=2.1.0->tldextract->scrapy) (4.0.0)\n",
      "Building wheels for collected packages: PyDispatcher\n",
      "  Building wheel for PyDispatcher (setup.py): started\n",
      "  Building wheel for PyDispatcher (setup.py): finished with status 'done'\n",
      "  Created wheel for PyDispatcher: filename=PyDispatcher-2.0.5-py3-none-any.whl size=12547 sha256=52af0cc921642ce333748e48c561b26d86dbd02bc95113de19663c5fe4e8776b\n",
      "  Stored in directory: c:\\users\\asus\\appdata\\local\\pip\\cache\\wheels\\3c\\31\\7f\\d7d7b5f0b9bad841ed856138ff0c5ee2bf2e04dbeb413097c8\n",
      "Successfully built PyDispatcher\n",
      "Installing collected packages: w3lib, cssselect, twisted-iocpsupport, requests-file, parsel, jmespath, itemadapter, incremental, hyperlink, constantly, Automat, Twisted, tldextract, service-identity, queuelib, PyDispatcher, protego, itemloaders, scrapy\n",
      "Successfully installed Automat-20.2.0 PyDispatcher-2.0.5 Twisted-22.1.0 constantly-15.1.0 cssselect-1.1.0 hyperlink-21.0.0 incremental-21.3.0 itemadapter-0.4.0 itemloaders-1.0.4 jmespath-0.10.0 parsel-1.6.0 protego-0.2.1 queuelib-1.6.2 requests-file-1.5.1 scrapy-2.6.1 service-identity-21.1.0 tldextract-3.2.0 twisted-iocpsupport-1.0.2 w3lib-1.22.0\n"
     ]
    }
   ],
   "source": [
    "!pip install scrapy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7254535",
   "metadata": {},
   "source": [
    "stocks.py文件源代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d8a5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import scrapy\n",
    "import re\n",
    "\n",
    "\n",
    "class StocksSpider(scrapy.Spider):\n",
    "    name = \"stocks\"\n",
    "    start_urls = ['https://quote.eastmoney.com/stocklist.html']#初始链接\n",
    "\n",
    "    def parse(self, response):\n",
    "        for href in response.css('a::attr(href)').extract():#提取其中所有的a标签，并从href属性中获取链接\n",
    "            try:\n",
    "                stock = re.findall(r\"[s][hz]\\d{6}\", href)[0]\n",
    "                url = 'https://gupiao.baidu.com/stock/' + stock + '.html'#生成百度股票的html网页\n",
    "                yield scrapy.Request(url, callback=self.parse_stock)\n",
    "                #根据生成的网页再生成相关的request对象，以供downloader进行下载\n",
    "                #callback是指定相应的函数类型\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "    def parse_stock(self, response):\n",
    "        infoDict = {}#ITEM pipeline是用一种类字典的形式处理，先生成一个空字典\n",
    "        stockInfo = response.css('.stock-bets')#找到相应的具有'stock-bets'属性的标签，返回的是selector对象\n",
    "        name = stockInfo.css('.bets-name').extract()[0]#借用上面的selector对象，返回所有的具有'bet-name'属性的标签的值组成的列表\n",
    "        keyList = stockInfo.css('dt').extract()#返回'dt'标签的值组成的列表\n",
    "        valueList = stockInfo.css('dd').extract()\n",
    "        for i in range(len(keyList)):\n",
    "            key = re.findall(r'>.*</dt>', keyList[i])[0][1:-5]\n",
    "            try:\n",
    "                val = re.findall(r'\\d+\\.?.*</dd>', valueList[i])[0][0:-5]\n",
    "            except:\n",
    "                val = '--'\n",
    "            infoDict[key]=val\n",
    "\n",
    "        infoDict.update(\n",
    "            {'股票名称': re.findall('\\s.*\\(',name)[0].split()[0] + \\\n",
    "             re.findall('\\>.*\\<', name)[0][1:-1]})#更新股票的名称\n",
    "        yield infoDict#将生成的item给itemPipeLine进行处理\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f03e06d",
   "metadata": {},
   "source": [
    "pipelines.py文件源代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979ea49f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "# Define your item pipelines here\n",
    "#\n",
    "# Don't forget to add your pipeline to the ITEM_PIPELINES setting\n",
    "# See: https://doc.scrapy.org/en/latest/topics/item-pipeline.html\n",
    "\n",
    "\n",
    "class BaidustocksPipeline(object):\n",
    "    def process_item(self, item, spider):\n",
    "        return item\n",
    "\n",
    "    #定义一个专门用来处理的infoDict的方法（对应前面的实例中写入文件的过程）\n",
    "class BaidustocksInfoPipeline(object):\n",
    "    def open_spider(self, spider):\n",
    "        self.f = open('BaiduStockInfo.txt', 'w')\n",
    "\n",
    "    def close_spider(self, spider):\n",
    "        self.f.close()\n",
    "\n",
    "    def process_item(self, item, spider):#核心部分，对每一个item的处理方法\n",
    "        try:\n",
    "            line = str(dict(item)) + '\\n'\n",
    "            self.f.write(line)\n",
    "        except:\n",
    "            pass\n",
    "        return item#如果需要别的函数也可以操作对应的item，就需要返回item对象"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ec4a30",
   "metadata": {},
   "source": [
    "settings.py文件中被修改的区域："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0710bb66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure item pipelines\n",
    "# See https://scrapy.readthedocs.org/en/latest/topics/item-pipeline.html\n",
    "ITEM_PIPELINES = {\n",
    "    'BaiduStocks.pipelines.BaidustocksInfoPipeline': 300,\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
